{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cGAN AI_ML Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MAhNgCIKWFU",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iP1wVN4p1XS",
        "colab_type": "code",
        "outputId": "777fd616-27c0-417f-d58a-dd255902f172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision>=0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0) (6.2.2)\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/6a/30d21c886293cca3755b8e55de34137a5068b77eba1c0644d3632080516b/Pillow-SIMD-7.0.0.post3.tar.gz (630kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110320 sha256=a9356a06fa7aaee482ac8bacc34d5f074a920d9cf101a0bf6abca609cc282340\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQHVxuCLyJ0T",
        "colab_type": "text"
      },
      "source": [
        "**Import LFWCrop Dataset from GitHub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQYSfKdbyJP-",
        "colab_type": "code",
        "outputId": "0682ae06-d450-40d4-897d-05df105e7cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import os\n",
        "!rm -rf lfwcrop\n",
        "!rm -rf output\n",
        "!git clone https://github.com/Antonio210696/LFWCrop_dataset_pytorch lfwcrop\n",
        "os.chdir('lfwcrop')\n",
        "!mv lfwcrop2.py ..\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lfwcrop'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 13361 (delta 45), reused 51 (delta 21), pack-reused 13286\u001b[K\n",
            "Receiving objects: 100% (13361/13361), 156.27 MiB | 40.54 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "Checking out files: 100% (13283/13283), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k-x7_ZYAnkn",
        "colab_type": "code",
        "outputId": "ec7184d0-4260-4ad7-e1c8-70f029332c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4X7wvjtKQuj",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s31cwgvn0JmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np  \n",
        "import time\n",
        "import random\n",
        "\n",
        "from lfwcrop2 import LFWCrop\n",
        "\n",
        "import torch \n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchvision import datasets \n",
        "from torchvision.datasets.utils import download_file_from_google_drive\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torchvision.utils as vutils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwhSyAHDenxB",
        "colab_type": "text"
      },
      "source": [
        "**Useful Classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpyZffBQep9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape layer\n",
        "class Reshape(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "    super(Reshape, self).__init__()\n",
        "    self.shape = args\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return x.view(self.shape)\n",
        "\n",
        "\n",
        "class ListModule(object):\n",
        "  def __init__(self, module, prefix, *args):\n",
        "    self.module = module\n",
        "    self.prefix = prefix    # prefix of the name of the list of modules\n",
        "    self.num_module = 0     # keeps count of the number of modules\n",
        "\n",
        "    # to start with predefined modules\n",
        "    for new_module in args:\n",
        "      self.append(new_module)\n",
        "\n",
        "  # to append new modules\n",
        "  def append(self, new_module):\n",
        "    if not isinstance(new_module, nn.Module):\n",
        "      raise ValueError('Not a Module')\n",
        "    else:\n",
        "      self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
        "      self.num_module += 1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.num_module\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    if i < 0 or i >= self.num_module:\n",
        "      raise IndexError('Out of bound')\n",
        "    return getattr(self.module, self.prefix + str(i))\n",
        "\n",
        "\n",
        "class MaxoutUnit(nn.Module):\n",
        "  def __init__(self, num_units=500):\n",
        "    super(MaxoutUnit, self).__init__()\n",
        "    self.fc1_list = ListModule(self, \"fc1_\")\n",
        "    self.fc2_list = ListModule(self, \"fc2_\")\n",
        "    self.dropout = nn.Dropout()\n",
        "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    for _ in range(num_units):\n",
        "      # num_pieces è uguale a 5 - per ora messi solo 2\n",
        "      self.fc1_list.append(nn.Linear(8 * 8 * 32, 1024)) # size uguale all'output di maxoutconv\n",
        "      self.fc2_list.append(nn.Linear(1024, 10))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 8 * 8 * 32) # size uguale all'output di maxoutconv\n",
        "    x = self.maxout(x, self.fc1_list)\n",
        "    x = self.dropout(x)\n",
        "    x = self.maxout(x, self.fc2_list)\n",
        "    return self.logsoftmax(x)\n",
        "  \n",
        "  def maxout(self, x, layer_list):\n",
        "    max_output = layer_list[0](x)\n",
        "    for _, layer in enumerate(layer_list, start=1):\n",
        "      max_output = torch.max(max_output, layer(x))\n",
        "    return max_output\n",
        "\n",
        "\n",
        "class MaxoutConv(nn.Module):\n",
        "  def __init__(self, num_units=1, in_channels=3):\n",
        "    super(MaxoutConv, self).__init__()\n",
        "    self.conv1_list = ListModule(self, \"conv1_\")\n",
        "    self.conv2_list = ListModule(self, \"conv2_\")\n",
        "    # self.conv3_list = ListModule(self, \"conv3_\")\n",
        "    \n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=4, stride=2)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=4, stride=2)\n",
        "    # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "\n",
        "    for _ in range(num_units):\n",
        "      self.conv1_list.append(nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=7, padding=4))\n",
        "      self.conv2_list.append(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=7, padding=4))\n",
        "      # self.conv3_list.append(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=3))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3, 32, 32)\n",
        "    x = self.pool1(self.maxout(x, self.conv1_list))\n",
        "    x = self.pool2(self.maxout(x, self.conv2_list)) # should be of size batch_size * 8 * 8 * 32 (16 * 16 * 32)\n",
        "    # x = self.pool3(self.maxout(x, self.conv3_list)) # should be of size batch_size * 17 * 17 * 64\n",
        "    return x    \n",
        "  \n",
        "  def maxout(self, x, layer_list):\n",
        "    max_output = layer_list[0](x)\n",
        "    for _, layer in enumerate(layer_list, start=1):\n",
        "      max_output = torch.max(max_output, layer(x))\n",
        "    return max_output\n",
        "\n",
        "preferred_dataset = 'lfwcrop'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMTM84YpeUgo",
        "colab_type": "text"
      },
      "source": [
        "**Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StFz9raLeZ5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator Class\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, n_classes, latentdim, batch_size, dataset_name, img_shape):\n",
        "    super(Generator, self).__init__()\n",
        "    self.label_embed = nn.Embedding(n_classes, n_classes)\n",
        "    self.dataset_name = dataset_name\n",
        "    self.img_shape = img_shape\n",
        "    self.depth = 8000\n",
        "\n",
        "    self.generator = nn.Sequential(\n",
        "        nn.Linear(latentdim + n_classes, self.depth),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.depth, self.depth),\n",
        "        nn.Sigmoid())\n",
        "    self.generator2=nn.Sequential(\n",
        "        nn.ConvTranspose2d(80, 40, 10, 6, bias=False),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(40, 3, 6, 2,2, bias=False),\n",
        "        )\n",
        "  \n",
        "  def forward(self, noise, labels, b_size):\n",
        "    if self.dataset_name != preferred_dataset:\n",
        "      print(\"Requested labels\", labels.size(), labels)\n",
        "      gen_input = torch.cat((self.label_embed(labels), noise), -1)\n",
        "      print(\"conditional vector size\", self.label_embed(labels).size())\n",
        "      print(\"Input to generator size\", gen_input.size())\n",
        "    else:\n",
        "      gen_input = torch.cat((labels, noise), -1)\n",
        "    \n",
        "    img1 = self.generator(gen_input).view(b_size, 80,10,10)\n",
        "    img = self.generator2(img1)\n",
        "    img = img.view(img.size(0), *self.img_shape)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IC0_ckPeXr3",
        "colab_type": "text"
      },
      "source": [
        "**Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YOM2dBBDnyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_classes, latentdim, batch_size, img_shape, dataset_name, ndf=64, nc=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.depth = 32*32*nc\n",
        "        self.dataset_name = dataset_name\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(192*4*4+n_classes, 192*4*4+n_classes),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(192*4*4+n_classes, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "           # nn.Conv2d(nc,nc, 4, 2, 1, bias=False),\n",
        "            #nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf, 192, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "           \n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels, batch_size):\n",
        "        inpu = self.main(img).view(batch_size, -1)\n",
        "        inpu = torch.cat((inpu, labels.float()), -1)\n",
        "        validity=self.linear(inpu)\n",
        "        return validity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LS8UND4Ke96",
        "colab_type": "text"
      },
      "source": [
        "**Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWBbQhPwKfr8",
        "colab_type": "code",
        "outputId": "86b7efb2-a5d3-4d3d-d72b-080c75418b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset_name = 'lfwcrop'                            # cifar10 or lsun or mnist\n",
        "dataroot = \"/content/\"                              # path to data \n",
        "lfwroot = '/content/lfwcrop/'\n",
        "\n",
        "BATCH_SIZE = 128           # input batch size, default 64\n",
        "IMG_SIZE = 32             # image size input, default 32\n",
        "CHANNELS = 3              # number of channels, default 1\n",
        "LAT_DIM = 100             # size of latent vector, default 100 dimensione del latent vector, che dovrebbe essere il rumore bianco in input https://ai.stackexchange.com/questions/12499/why-is-it-called-latent-vector\n",
        "PATH_g=\"./drive/My Drive/AI_ML_project_data/output3_6/\"\n",
        "PATH_d=\"./drive/My Drive/AI_ML_project_data/output3_6/\" \n",
        "N_CLASSES = 11             # number of classes in data set, default 10 (10 digits) for cifar10, mnist and lsun. For celeb 40, for lfwcrop 37\n",
        "\n",
        "EPOCHS = 2000             # number of epoch default 200\n",
        "LR = 2e-4                 # learning  rate default 0.0002\n",
        "\n",
        "B = 0.5                   # beta for adam optimizer default 0.5\n",
        "B1 = 0.999                # beta1 for adam optimizer default 0.999\n",
        "\n",
        "outputdir = \"./drive/My Drive/AI_ML_project_data/output3_6/\"   # folder to output images and model checkpoints \n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 345\n",
        "\n",
        "# manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff1d711a290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-29yP4dNLD7v",
        "colab_type": "text"
      },
      "source": [
        "**Define data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krdfoj2hKOtq",
        "colab_type": "code",
        "outputId": "de03a0ae-eab3-4fde-abbc-ab3539fc70a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False \n",
        "\n",
        "os.makedirs(outputdir, exist_ok=True)\n",
        "\n",
        "randomseed = random.randint(1, 10000)\n",
        "random.seed(randomseed)\n",
        "torch.manual_seed(randomseed)\n",
        "\n",
        "# preprocessing for mnist, lsun, cifar10\n",
        "if dataset_name == 'mnist': \n",
        "\tdataset = datasets.MNIST(root = dataroot, train=True,download=True, \n",
        "\t\ttransform=transforms.Compose([transforms.Resize(IMG_SIZE), \n",
        "\t\t\ttransforms.ToTensor(), \n",
        "\t\t\ttransforms.Normalize((0.5,), (0.5,))]))\n",
        "\n",
        "elif dataset_name == 'lsun': \n",
        "\tdataset = datasets.LSUN(root = dataroot, train=True,download=True, \n",
        "\t\ttransform=transforms.Compose([transforms.Resize(IMG_SIZE), \n",
        "\t\t\ttransforms.CenterCrop(IMG_SIZE),\n",
        "\t\t\ttransforms.ToTensor(), \n",
        "\t\t\ttransforms.Normalize((0.5,), (0.5,))]))\n",
        "\n",
        "elif dataset_name == 'cifar10':  \n",
        "\tdataset = datasets.CIFAR10(root = dataroot, train=True,download=True, \n",
        "\t\ttransform=transforms.Compose([transforms.Resize(IMG_SIZE), \n",
        "\t\t\ttransforms.ToTensor(), \n",
        "\t\t\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        " \n",
        "elif dataset_name == 'celeb': \n",
        "\tdataset = datasets.CelebA(root = dataroot, split='train',download=False, target_type='attr', \n",
        "\t\ttransform=transforms.Compose([\n",
        "\t\t\t#transforms.Resize((IMG_SIZE*2, IMG_SIZE)), \n",
        "\t\t  transforms.CenterCrop(IMG_SIZE*3),\n",
        "\t\t\ttransforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
        "\t\t\ttransforms.ToTensor(), \n",
        "\t\t\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "elif dataset_name == 'lfwcrop': \n",
        "\tdataset = LFWCrop(lfwroot,pca_components=0.70, transform=transforms.Compose([   #, pca_components=0.70\n",
        "\t\t\t# transforms.Resize((IMG_SIZE*2, IMG_SIZE)), \n",
        "\t\t\ttransforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
        "\t\t\ttransforms.ToTensor()\n",
        "\t\t\t# transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\t ]))\n",
        "\n",
        "assert dataset\n",
        "X, labels = dataset.__getitem__(200)\n",
        "print(\"labels are of type\" + str(type(labels)))\n",
        "plt.imshow(X.permute(1, 2, 0))\n",
        "print(\"Attribute list for shown image\", labels)\n",
        "print(\"Number of images\", len(dataset))\n",
        "\n",
        "# il train contiene 162.000 immagini circa. Ne analizziamo n_sample scelte randomicamente\n",
        "n_sample = dataset.__len__()\n",
        "train_indices = (np.random.randint(0, len(dataset), n_sample))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(train_indices)) # senza il sampler mettere shuffle=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using %d attributes 73\n",
            "Root directory is /content/lfwcrop/\n",
            "Selected 13233 actors\n",
            "personis not in attributes to use\n",
            "imagenumis not in attributes to use\n",
            "Maleis in attributes to use\n",
            "Asianis in attributes to use\n",
            "Whiteis in attributes to use\n",
            "Blackis in attributes to use\n",
            "Babyis in attributes to use\n",
            "Childis in attributes to use\n",
            "Youthis in attributes to use\n",
            "Middle Agedis in attributes to use\n",
            "Senioris in attributes to use\n",
            "Black Hairis in attributes to use\n",
            "Blond Hairis in attributes to use\n",
            "Brown Hairis in attributes to use\n",
            "Baldis in attributes to use\n",
            "No Eyewearis in attributes to use\n",
            "Eyeglassesis in attributes to use\n",
            "Sunglassesis in attributes to use\n",
            "Mustacheis in attributes to use\n",
            "Smilingis in attributes to use\n",
            "Frowningis in attributes to use\n",
            "Chubbyis in attributes to use\n",
            "Blurryis in attributes to use\n",
            "Harsh Lightingis in attributes to use\n",
            "Flashis in attributes to use\n",
            "Soft Lightingis in attributes to use\n",
            "Outdooris in attributes to use\n",
            "Curly Hairis in attributes to use\n",
            "Wavy Hairis in attributes to use\n",
            "Straight Hairis in attributes to use\n",
            "Receding Hairlineis in attributes to use\n",
            "Bangsis in attributes to use\n",
            "Sideburnsis in attributes to use\n",
            "Fully Visible Foreheadis in attributes to use\n",
            "Partially Visible Foreheadis in attributes to use\n",
            "Obstructed Foreheadis in attributes to use\n",
            "Bushy Eyebrowsis in attributes to use\n",
            "Arched Eyebrowsis in attributes to use\n",
            "Narrow Eyesis in attributes to use\n",
            "Eyes Openis in attributes to use\n",
            "Big Noseis in attributes to use\n",
            "Pointy Noseis in attributes to use\n",
            "Big Lipsis in attributes to use\n",
            "Mouth Closedis in attributes to use\n",
            "Mouth Slightly Openis in attributes to use\n",
            "Mouth Wide Openis in attributes to use\n",
            "Teeth Not Visibleis in attributes to use\n",
            "No Beardis in attributes to use\n",
            "Goateeis in attributes to use\n",
            "Round Jawis in attributes to use\n",
            "Double Chinis in attributes to use\n",
            "Wearing Hatis in attributes to use\n",
            "Oval Faceis in attributes to use\n",
            "Square Faceis in attributes to use\n",
            "Round Faceis in attributes to use\n",
            "Color Photois in attributes to use\n",
            "Posed Photois in attributes to use\n",
            "Attractive Manis in attributes to use\n",
            "Attractive Womanis in attributes to use\n",
            "Indianis in attributes to use\n",
            "Gray Hairis in attributes to use\n",
            "Bags Under Eyesis in attributes to use\n",
            "Heavy Makeupis in attributes to use\n",
            "Rosy Cheeksis in attributes to use\n",
            "Shiny Skinis in attributes to use\n",
            "Pale Skinis in attributes to use\n",
            "5 o' Clock Shadowis in attributes to use\n",
            "Strong Nose-Mouth Linesis in attributes to use\n",
            "Wearing Lipstickis in attributes to use\n",
            "Flushed Faceis in attributes to use\n",
            "High Cheekbonesis in attributes to use\n",
            "Brown Eyesis in attributes to use\n",
            "Wearing Earringsis in attributes to use\n",
            "Wearing Necktieis in attributes to use\n",
            "Wearing Necklaceis in attributes to use\n",
            "is not in attributes to use\n",
            "We have 73\n",
            "(11, 73)\n",
            "           Male     Asian     White     Black      Baby     Child     Youth  \\\n",
            "PC-1  -0.320865 -0.004433 -0.006903 -0.038995 -0.019663  0.044120  0.140204   \n",
            "PC-2   0.011656  0.016815 -0.047997  0.012823  0.004693  0.047578  0.022798   \n",
            "PC-3   0.014032  0.109513 -0.297789  0.156386 -0.020493  0.030681  0.041158   \n",
            "PC-4  -0.073076 -0.035031  0.020406 -0.054012  0.029901  0.115309  0.055055   \n",
            "PC-5  -0.107631  0.168057 -0.064259 -0.005390  0.098808  0.108049 -0.051205   \n",
            "PC-6  -0.131367  0.214880 -0.132972 -0.012485 -0.046670 -0.003194  0.018976   \n",
            "PC-7   0.083549  0.254895 -0.296988 -0.047991  0.010427 -0.004383  0.057988   \n",
            "PC-8  -0.051536 -0.119426 -0.049998  0.266532  0.045839 -0.023655 -0.040423   \n",
            "PC-9  -0.024184 -0.048202  0.092002 -0.126864  0.007926 -0.034319 -0.008244   \n",
            "PC-10 -0.127654 -0.020929 -0.076312  0.191052  0.161181  0.123323  0.002725   \n",
            "PC-11  0.070146  0.030571  0.127261 -0.219398  0.188711  0.016814 -0.132570   \n",
            "\n",
            "       Middle Aged    Senior  Black Hair  Blond Hair  Brown Hair      Bald  \\\n",
            "PC-1     -0.035460 -0.117761    0.023753    0.063424    0.099511 -0.121505   \n",
            "PC-2      0.000504 -0.051404    0.051707   -0.002287    0.042816 -0.023028   \n",
            "PC-3      0.005376 -0.048909    0.162001   -0.060260    0.043235  0.002322   \n",
            "PC-4     -0.023581 -0.101822    0.028442    0.031234    0.069005 -0.074453   \n",
            "PC-5     -0.065436  0.108755   -0.049028    0.041177   -0.133443 -0.012254   \n",
            "PC-6     -0.058201  0.096392   -0.000394   -0.018458   -0.138201  0.043181   \n",
            "PC-7      0.024754 -0.144132    0.249551   -0.103226    0.145787 -0.043154   \n",
            "PC-8      0.004853  0.071563   -0.103552    0.020006   -0.276846  0.163326   \n",
            "PC-9      0.037116 -0.018459    0.059213   -0.011009    0.618230 -0.180461   \n",
            "PC-10    -0.019423 -0.075592    0.062312    0.015207   -0.084530  0.011025   \n",
            "PC-11     0.009876  0.011205   -0.001648   -0.009199   -0.117053 -0.011781   \n",
            "\n",
            "       No Eyewear  Eyeglasses  Sunglasses  Mustache   Smiling  Frowning  \\\n",
            "PC-1     0.166474   -0.144614   -0.028121 -0.147277  0.232829 -0.239683   \n",
            "PC-2     0.031462   -0.061079    0.022241  0.019720 -0.094969  0.099094   \n",
            "PC-3    -0.011770   -0.013570    0.033454  0.032485  0.094059 -0.095199   \n",
            "PC-4     0.091881   -0.126552    0.009160 -0.031642 -0.348589  0.359033   \n",
            "PC-5    -0.281284    0.156218    0.071555 -0.092392 -0.101108  0.100034   \n",
            "PC-6    -0.341298    0.301422   -0.027907 -0.151464 -0.067048  0.069948   \n",
            "PC-7     0.019675    0.078467   -0.087697  0.086228 -0.008913  0.012820   \n",
            "PC-8    -0.267340    0.190295    0.066622  0.119510  0.023528 -0.033944   \n",
            "PC-9    -0.407371    0.266242    0.080647  0.008869  0.022683 -0.033807   \n",
            "PC-10    0.145451   -0.163437    0.045213  0.062052 -0.103906  0.099602   \n",
            "PC-11    0.095516    0.054902   -0.108831 -0.019349 -0.034081  0.028240   \n",
            "\n",
            "         Chubby    Blurry  Harsh Lighting     Flash  Soft Lighting   Outdoor  \\\n",
            "PC-1  -0.086403  0.004539       -0.096906  0.027164       0.039842 -0.023124   \n",
            "PC-2  -0.027662  0.006954        0.034720 -0.059583       0.063970  0.058469   \n",
            "PC-3  -0.016168  0.035709        0.114787 -0.030345      -0.060028  0.036427   \n",
            "PC-4  -0.083920 -0.011180       -0.035916 -0.047379       0.006779 -0.039874   \n",
            "PC-5   0.120043  0.037317        0.130476 -0.127200      -0.032214  0.182495   \n",
            "PC-6  -0.020867  0.030023       -0.110879  0.075850       0.069670 -0.124288   \n",
            "PC-7   0.004504 -0.038504       -0.206487  0.179238       0.012671 -0.177702   \n",
            "PC-8  -0.087172 -0.024745        0.046023  0.205899      -0.149669 -0.202589   \n",
            "PC-9  -0.011658 -0.024920        0.192050  0.044631      -0.290127 -0.038900   \n",
            "PC-10  0.074129 -0.031302        0.368618 -0.051550      -0.451740  0.063243   \n",
            "PC-11  0.231849 -0.092024       -0.193502  0.101201      -0.065536 -0.159651   \n",
            "\n",
            "       Curly Hair  Wavy Hair  Straight Hair  Receding Hairline     Bangs  \\\n",
            "PC-1    -0.091560   0.036604       0.003787          -0.253815  0.029032   \n",
            "PC-2     0.011596  -0.027233       0.001490          -0.057636  0.030386   \n",
            "PC-3    -0.058116  -0.129027       0.160230          -0.109932  0.078999   \n",
            "PC-4    -0.001724   0.019853      -0.045554          -0.176716  0.058589   \n",
            "PC-5     0.032313  -0.034531       0.006917          -0.119858  0.217355   \n",
            "PC-6    -0.069911   0.035774       0.103094           0.088572 -0.236398   \n",
            "PC-7    -0.145073  -0.197892       0.320871          -0.153228 -0.008425   \n",
            "PC-8    -0.074652   0.133500      -0.062237           0.099838  0.102699   \n",
            "PC-9     0.139761  -0.060571      -0.154135          -0.117228  0.013321   \n",
            "PC-10   -0.002579   0.017326      -0.038333          -0.068884 -0.196099   \n",
            "PC-11    0.018144  -0.061527       0.028772          -0.034561 -0.019071   \n",
            "\n",
            "       Sideburns  Fully Visible Forehead  Partially Visible Forehead  \\\n",
            "PC-1   -0.090861               -0.097330                    0.029062   \n",
            "PC-2    0.012685               -0.059461                    0.010421   \n",
            "PC-3   -0.002048               -0.127960                    0.021285   \n",
            "PC-4    0.001191               -0.155551                    0.029081   \n",
            "PC-5   -0.068560               -0.407138                    0.090934   \n",
            "PC-6   -0.054124                0.417114                   -0.085623   \n",
            "PC-7    0.034804               -0.096202                   -0.006683   \n",
            "PC-8   -0.004630               -0.204570                    0.037425   \n",
            "PC-9    0.009390               -0.060605                    0.022764   \n",
            "PC-10  -0.049543                0.360650                   -0.076991   \n",
            "PC-11   0.005917               -0.069381                   -0.012536   \n",
            "\n",
            "       Obstructed Forehead  Bushy Eyebrows  Arched Eyebrows  Narrow Eyes  \\\n",
            "PC-1             -0.025248       -0.155596         0.177879    -0.117333   \n",
            "PC-2              0.029525        0.015967        -0.022726    -0.008253   \n",
            "PC-3              0.075360        0.083712         0.043358     0.046957   \n",
            "PC-4              0.050547       -0.043130         0.009676    -0.183413   \n",
            "PC-5              0.207501       -0.063245         0.084453     0.237947   \n",
            "PC-6             -0.180169       -0.050614         0.145612     0.088201   \n",
            "PC-7             -0.036754        0.144334         0.169546    -0.002492   \n",
            "PC-8              0.072890        0.004949         0.266180    -0.410199   \n",
            "PC-9             -0.076374        0.000986        -0.076873    -0.055787   \n",
            "PC-10            -0.117182       -0.051224         0.073441     0.053414   \n",
            "PC-11             0.006831       -0.016232        -0.013472     0.023506   \n",
            "\n",
            "       Eyes Open  Big Nose  Pointy Nose  Big Lips  Mouth Closed  \\\n",
            "PC-1    0.080524 -0.086737     0.029819  0.017839     -0.145664   \n",
            "PC-2   -0.009393 -0.027999    -0.001702  0.035813      0.071098   \n",
            "PC-3   -0.084268  0.090062    -0.118705  0.089415     -0.097808   \n",
            "PC-4    0.115176 -0.192512     0.106535  0.067187      0.222516   \n",
            "PC-5   -0.188050 -0.049109    -0.111287  0.021923     -0.015275   \n",
            "PC-6   -0.016513 -0.045373    -0.049572  0.020503      0.131409   \n",
            "PC-7    0.059412 -0.019399    -0.060677  0.088564      0.009932   \n",
            "PC-8    0.146755  0.022315     0.072482  0.060007     -0.016961   \n",
            "PC-9   -0.012872  0.003075     0.039345 -0.056358      0.017088   \n",
            "PC-10  -0.152279 -0.071506    -0.001280  0.169077     -0.117326   \n",
            "PC-11   0.040048 -0.166252     0.068033 -0.055350     -0.434478   \n",
            "\n",
            "       Mouth Slightly Open  Mouth Wide Open  Teeth Not Visible  No Beard  \\\n",
            "PC-1              0.114626         0.000925          -0.167188  0.199838   \n",
            "PC-2             -0.048185        -0.009500           0.079070 -0.029281   \n",
            "PC-3              0.048227         0.030950          -0.085483 -0.031988   \n",
            "PC-4             -0.138135        -0.046801           0.263681  0.051107   \n",
            "PC-5             -0.044928         0.060064           0.062693  0.107478   \n",
            "PC-6             -0.035230        -0.080510           0.088766  0.178847   \n",
            "PC-7             -0.004421        -0.012912           0.004726 -0.097038   \n",
            "PC-8              0.003131         0.023377          -0.056085 -0.120585   \n",
            "PC-9             -0.005151        -0.012932          -0.019883 -0.013847   \n",
            "PC-10            -0.021483         0.128956          -0.012224 -0.034338   \n",
            "PC-11             0.127648         0.217417          -0.150123  0.030823   \n",
            "\n",
            "         Goatee  Round Jaw  Double Chin  Wearing Hat  Oval Face  Square Face  \\\n",
            "PC-1  -0.122793   0.029251    -0.067859    -0.040300   0.054799    -0.061388   \n",
            "PC-2   0.014083   0.006241    -0.036646     0.023049  -0.011572    -0.001940   \n",
            "PC-3   0.000685   0.017255    -0.029074     0.047639  -0.046883    -0.007201   \n",
            "PC-4  -0.006485   0.007070    -0.083834     0.042571   0.058946    -0.023025   \n",
            "PC-5  -0.103364   0.042195     0.057703     0.180406  -0.120186    -0.030836   \n",
            "PC-6  -0.103131   0.003323    -0.012548    -0.115084   0.017109    -0.010938   \n",
            "PC-7   0.044208  -0.005062    -0.003585    -0.050286  -0.037692    -0.003429   \n",
            "PC-8   0.065283  -0.012867     0.004516     0.101235   0.113033    -0.029654   \n",
            "PC-9   0.000090  -0.011176     0.038505    -0.172380  -0.004727     0.022997   \n",
            "PC-10 -0.008183   0.037890     0.046180    -0.041251  -0.035659    -0.024740   \n",
            "PC-11 -0.011704  -0.002185     0.172684     0.062147  -0.119521     0.000438   \n",
            "\n",
            "       Round Face  Color Photo  Posed Photo  Attractive Man  Attractive Woman  \\\n",
            "PC-1     0.021072    -0.037946     0.064458        0.083880          0.141572   \n",
            "PC-2     0.012483    -0.622132    -0.021716        0.020126          0.025244   \n",
            "PC-3     0.033816     0.209420    -0.048661       -0.033479         -0.019159   \n",
            "PC-4    -0.001185     0.323800     0.008416        0.066680          0.094044   \n",
            "PC-5     0.094292    -0.085991    -0.128791       -0.130489         -0.036587   \n",
            "PC-6    -0.004067     0.124264     0.014261       -0.113651         -0.010050   \n",
            "PC-7     0.024917    -0.170522     0.036100       -0.048864         -0.053250   \n",
            "PC-8    -0.028565    -0.122911     0.035955       -0.013872          0.057631   \n",
            "PC-9    -0.031973     0.053414     0.042286        0.023999         -0.028206   \n",
            "PC-10    0.053134    -0.157519    -0.093766       -0.019246          0.085170   \n",
            "PC-11    0.076772     0.184230    -0.075278       -0.090823         -0.107838   \n",
            "\n",
            "         Indian  Gray Hair  Bags Under Eyes  Heavy Makeup  Rosy Cheeks  \\\n",
            "PC-1  -0.005764  -0.122626        -0.086460      0.177877     0.033400   \n",
            "PC-2   0.024504  -0.039498        -0.011803     -0.007303    -0.425836   \n",
            "PC-3   0.085881  -0.043484         0.042242     -0.034854    -0.393981   \n",
            "PC-4   0.012104  -0.079048        -0.144995      0.083622    -0.015431   \n",
            "PC-5  -0.052154   0.073976         0.032665      0.028655     0.213399   \n",
            "PC-6  -0.081085   0.028712        -0.044586      0.075465    -0.099327   \n",
            "PC-7   0.025631  -0.099348        -0.103866     -0.042252     0.235307   \n",
            "PC-8   0.067110   0.070682        -0.086643      0.085129    -0.089532   \n",
            "PC-9   0.011739  -0.076290        -0.000785      0.024199    -0.164633   \n",
            "PC-10  0.011503  -0.015159        -0.089820      0.064065    -0.022120   \n",
            "PC-11 -0.099515   0.043250        -0.131266     -0.022776    -0.356038   \n",
            "\n",
            "       Shiny Skin  Pale Skin  5 o' Clock Shadow  Strong Nose-Mouth Lines  \\\n",
            "PC-1     0.043793   0.085753          -0.239082                 0.055732   \n",
            "PC-2    -0.045779   0.229567           0.033131                -0.064026   \n",
            "PC-3     0.017599  -0.572367           0.017991                 0.076019   \n",
            "PC-4    -0.084221  -0.190696          -0.002052                -0.299054   \n",
            "PC-5    -0.060552   0.113753          -0.215223                 0.019354   \n",
            "PC-6     0.024683  -0.018916          -0.216468                -0.085600   \n",
            "PC-7     0.077776   0.225897           0.147325                -0.185093   \n",
            "PC-8     0.157985  -0.047191           0.103442                -0.054215   \n",
            "PC-9     0.032383   0.074496           0.018483                 0.019015   \n",
            "PC-10    0.016000   0.045822          -0.024813                -0.085721   \n",
            "PC-11   -0.016577   0.076004          -0.080211                -0.154444   \n",
            "\n",
            "       Wearing Lipstick  Flushed Face  High Cheekbones  Brown Eyes  \\\n",
            "PC-1           0.259203     -0.067469         0.147626   -0.026468   \n",
            "PC-2          -0.020670     -0.529473        -0.065760    0.021025   \n",
            "PC-3          -0.026828     -0.112890         0.055145    0.258709   \n",
            "PC-4           0.051115     -0.043438        -0.206710   -0.034655   \n",
            "PC-5           0.052215      0.078885         0.005532    0.021196   \n",
            "PC-6           0.102883     -0.230416         0.008017    0.065094   \n",
            "PC-7          -0.044083      0.236251        -0.079157    0.093694   \n",
            "PC-8           0.088064      0.050089        -0.015467   -0.142076   \n",
            "PC-9           0.011061      0.063337         0.002419   -0.097210   \n",
            "PC-10          0.028875      0.264467        -0.070032    0.035590   \n",
            "PC-11         -0.075784     -0.054497        -0.095728   -0.200267   \n",
            "\n",
            "       Wearing Earrings  Wearing Necktie  Wearing Necklace  \n",
            "PC-1           0.146167        -0.134192          0.157484  \n",
            "PC-2          -0.021086        -0.028710         -0.016485  \n",
            "PC-3          -0.004250        -0.020843          0.021478  \n",
            "PC-4           0.019789        -0.059678         -0.032387  \n",
            "PC-5           0.025811        -0.059478          0.009338  \n",
            "PC-6           0.038158         0.008773         -0.021761  \n",
            "PC-7          -0.031067         0.019947         -0.074344  \n",
            "PC-8           0.059061         0.008166          0.006048  \n",
            "PC-9           0.003262         0.015340          0.032077  \n",
            "PC-10          0.034918        -0.069488         -0.000341  \n",
            "PC-11         -0.005401         0.040825         -0.071326  \n",
            "labels are of type<class 'torch.Tensor'>\n",
            "Attribute list for shown image tensor([ 0.9230,  4.8951,  6.8327,  1.3113, -1.0496,  1.0347, -2.6029,  3.5178,\n",
            "         0.1273,  0.8810,  1.0340])\n",
            "Number of images 13140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdL0lEQVR4nO2dbahl53Xf/2vvfd7uuW8zd8byWFIi\n2TUUExrZDMIlJrgJCaoJyIZi7A9GH0wmlAhikn4QLtQu9INTaht/KC7jWkQprl8a21gU08YRAZMP\nlT12ZUm22lgSsqXRaEbzcue+ndd9Vj+co3Qknv+6d+bee66S5/+DYc7d6zx7r/Psvc4+5/mftZa5\nO4QQ//ApjtoBIcR8ULALkQkKdiEyQcEuRCYo2IXIBAW7EJlQ7Wewmd0H4AsASgD/2d0/Ez2/a4Uf\nN/L+YnxcQd6TSrYvABbYomNZZAORKYNBkbDpwbEmwbgIdrxof5H6asFkRXPF93drx4ruSuE+iZNF\neKKD1xwcqwjmMbKx+a+DszYhgy6OBrhej5Ju3nKwm1kJ4D8C+B0ALwH4oZk96u4/Y2OOW4E/bi+T\n/fHT2UY7uX2x1aVjWq0WtVmDn7LS+FmpiK2ouO91YBsGV04vONE1H4ZxWab3N+GvazTmx2oYv0Qa\n5FhT0sergnBpFXx/zeD6aAT77JRp/5vNJh1TVtyPIpjHhXSMTW01t9V1+oxu1EM6ZnvcT25/8JdP\n0TH7+Rh/L4Bn3f15dx8C+BqA+/exPyHEIbKfYL8dwIs3/P3SbJsQ4k3IoS/QmdkZMztnZue2w2+w\nQojDZD/Bfh7AnTf8fcds2+tw97PuftrdT3fD5Q0hxGGyn2D/IYB3mtndZtYE8BEAjx6MW0KIg+aW\nV+PdfWxmDwL4n5hKbw+7+0+jMWZcLrMWd6Wq0iunVaNBx5Qlfx/zgn/CmARfNYZkhXw4HtExm5Mx\ntV2bDKjtIrhtpwxW6hvp190bcT8mQ76+X0wCOcyDOa6JchF8k+sElyNbVZ/a+Or5YiOtynTaHTqm\n2eG2JbI/AFgzvsK/5tz/LlEhFgruh1Xp/RUFPyf70tnd/bsAvruffQgh5oN+QSdEJijYhcgEBbsQ\nmaBgFyITFOxCZMK+VuNvlqIosbiYToQpA+mtURKJLZB+hkFCyzj4bc92IJVtTNKJCVfrHTrmMnrU\nds24vLZVBeku7UCm7BBJZsLlOhsGtmCOESXXEDlvHBwLdTq5AwCagbxWBXJTPUj7Ue9w38tA5ltt\nLFDbnbZEbXeXK9R2RycdE912OgEMALwgEmCQMKQ7uxCZoGAXIhMU7EJkgoJdiExQsAuRCXNfjW+T\nlUfyu34AgJGV5F6QVbFZ8NXsayNue3GwTW0vjbeS268Eq+q9Jl99rpaCpIq1VWq77S3Hqe34Snq1\neCGYq2ZQhK6qeLIRgpJV/WFa1egNuNqxQ8YAwDBY+e8NeSLS5Wsb6e1X1rkfW1xBsc30NQAAzw2u\nUduzBV+p/5XF9Lk+dZyf57XuYnL7IChnpju7EJmgYBciExTsQmSCgl2ITFCwC5EJCnYhMmGu0pvD\nUJOklrFz2aVG2nbVeceMX/a5HPZ8jydcvNDn0tslSx+vX3FZKKoJdiyQro51uFTTXuay3NJiOnli\nJTjTC6RuHQC0FngyhjV4zbUekdG2e/w890ZcNtoKZLmrG/ycVTWpeTjg56w/4ratIU96ujLi19z5\nQB58jsh5b2vwY91VriW37wRxpDu7EJmgYBciExTsQmSCgl2ITFCwC5EJCnYhMmFf0puZvQBgE0AN\nYOzup6Pn1wZcL9MF4DbGXEZb983k9lcnXEL7xQ7PXDo/5llvO0S6AgBrpjON2kFNu8mI+7i1zm3P\njV+mtuubPGPrFMl6e9sqr5321hNcyltt8ay3ViA5jsZpCej61at0zPoGP2dbPX7OXr7E9/nyxXTW\n2yCQ+TzIEGx0+Dx6h4/bDrL9xk7q9TX5fNTNtNzYtyDLklr2zj9z98sHsB8hxCGij/FCZMJ+g90B\n/KWZ/cjMzhyEQ0KIw2G/H+Pf5+7nzewtAL5nZv/H3b9/4xNmbwJnAOAYab0shDh89nVnd/fzs/8v\nAfg2gHsTzznr7qfd/fRiUHxfCHG43HKwm1nXbNoCw8y6AH4XwNMH5ZgQ4mDZz632NgDfNrPX9vNf\n3f1/RANGPsFL47SccLkMMoaK9JhrYy5djRa5ZLS2dILabl85Rm2TMv01ZExkJgDY3OYy2bWNK9TW\n2+KZXBeDopg76+n37+2NLh1TRK23VtIFQgEgUBwxJHNy8fKrdMz5l7lt7Px8rm/y62CbSLDNVoeO\neetJXuixvcylt6IRabBcOvRBOiOuLIKWV8QPe5bfv2852N39eQC/fqvjhRDzRdKbEJmgYBciExTs\nQmSCgl2ITFCwC5EJc/2Vy8gcF8p0dtvWIndli/Qb8wYf0y24tFIN+HvcYJP36xoM0/LJKJBVauOy\nXLfBpZXFgmffFeDFC4s67Utvixcv3LjK5cG11RVq6zS5HFaTTK6qxV9zo8Ntw6A/X7HIx7WRnsdA\nLcVWIG3WG1wiXl7gvxA9scozC9dOnkxubzT5/hrLaSn1f/3gBTpGd3YhMkHBLkQmKNiFyAQFuxCZ\noGAXIhPmuho/KQ29VbKC2+GurFbplVgLan4Nx7wW15WtYMW9x+t+LTbTK6CrXb7y313gCSiNir/m\nXlC7bmc73S4IAMYkOagZ1IsrJ3yuWoFtwXjihxVp22qXqwy9ZT6P46DFUx3knzS66TZatXElwZzf\nAxs1V0ImW9epbWfAr6u3VOnjvXX1rXRMczE9V42S+647uxCZoGAXIhMU7EJkgoJdiExQsAuRCQp2\nITJhrtKbNUpUt6UTK0rwzIQmqVvXmPCWUeOgDU7zZNDS6HYulZ1YStenW+rwMS2SxAMAddCG6kqQ\nuLJ+nWtNg146KaQ0fqzFNpfDGkGluSqQ5ZZapF7fIp8rG/HzuUj2BwB9/tLgzbRE1SSSHAB0Ax/b\nFsi9gfQ2HvDXtkzq4XWDyuutRtoP0l0NgO7sQmSDgl2ITFCwC5EJCnYhMkHBLkQmKNiFyIRdpTcz\nexjA7wG45O6/Ntt2HMDXAdwF4AUAH3Z3nkr22r7KAq3VtMww7vOsJttJSzzdgssgnWaL2lqBtLLQ\nDWQ0IpGUQS22Ebg8NRhy/5cb3H8E9fV2Gmkdqhlk2LU7XOMJEttgE+4/y746HsxvMzjWWlCDbhxI\ngLUTKbLF5cayzecqakTcOMGz1Grj91Ubpl94e8IPxq7FIjjOXu7sfwbgvjdsewjAY+7+TgCPzf4W\nQryJ2TXYZ/3Wr75h8/0AHpk9fgTABw/YLyHEAXOr39lvc/cLs8evYNrRVQjxJmbfC3Tu7gDolzcz\nO2Nm58zsXL/Pq3wIIQ6XWw32i2Z2CgBm/19iT3T3s+5+2t1Pt9t8IUsIcbjcarA/CuCB2eMHAHzn\nYNwRQhwWe5Hevgrg/QBOmNlLAD4F4DMAvmFmHwfwCwAf3svBDEBFpCj3oOghaRl0LEgLWukuUtvy\n0gI/VodLMkyHsipoW+Rcnto0XoRw1OdZUq1GoFEtpCW7hRaX8pqBdAj+0jCugx5KnvaxCloaLZf8\ncqyDfk1WBPcslnVY8hfm/FspypLbomvHK26beNpWgZ8zdgVYwV/XrsHu7h8lpt/ebawQ4s2DfkEn\nRCYo2IXIBAW7EJmgYBciExTsQmTCfAtOAmgS0YD1BgOALslQWlnh2V+ri8vUthhIb41ARhsP03JY\nHchr4yHP1ir6XE4qR9zWCeSVLskqKwJ5yp0faxwUgZyAZ7BRyavgl1yjGWRsBfMRCJEoyQ+5rAr6\n1AUycDAMzSaX16zJC1yiTF+rRRFIeZP0fJSBpKg7uxCZoGAXIhMU7EJkgoJdiExQsAuRCQp2ITJh\nrtIbABSkV1agvKFJZBy2HQCqoMBiFUhXZVCwz4ms4UHPtohWkIm2OObyT+l9aiuIj5G0ORoFhSOD\nuWoG/rdJQcTo/hKdlwbpbQYAQfs1VCWZx6BYaVlG1xX30aIioUEB1EaVnqtGxSXiuk5LopHEqju7\nEJmgYBciExTsQmSCgl2ITFCwC5EJc12Nd0wwrNMryRYsqbK6Wh69V0XZEVFPo2A1k63SFsEqspW8\n5lqzyVfcR0ECzeYWr11XkCSORlC3rgjmowr87zR4ItJCiyTJBCvndR20cRrzMuQlaTUFAIud9Ip2\ndYt19yy6sIqgjVYrSL6iCTSBMkTbivEJ1p1diExQsAuRCQp2ITJBwS5EJijYhcgEBbsQmbCX9k8P\nA/g9AJfc/ddm2z4N4PcBvDp72ifd/bu77WsymaDXT0tv3XYgW5AkjnrCZYY6kHg8kE/KKEmmSvsY\nJR8MgySZ8eYOtY2GXGoKSqShahBJKZA2a3AfG20urx1bO0FtC5209La9xV/z1sYmtQ0GfD6aFZ//\nSTudgGKkPRUAFBa0UCq4ZGcVT3YpGryeXEmSawZjXv9vNE7Lr1Hrqr3c2f8MwH2J7Z9393tm/3YN\ndCHE0bJrsLv79wFcnYMvQohDZD/f2R80syfN7GEzO3ZgHgkhDoVbDfYvAngHgHsAXADwWfZEMztj\nZufM7NxgELT4FUIcKrcU7O5+0d1rnzZV/xKAe4PnnnX30+5+utWae2EcIcSMWwp2Mzt1w58fAvD0\nwbgjhDgs9iK9fRXA+wGcMLOXAHwKwPvN7B5MU2xeAPAHezmYO5eieE4QMJ6ktabBmH8tGI64nDQK\ndLlm8P7XZNJbkDXmQ66T9Xq8ltw48H+hzdsudZfTbYYmrBYbgEYg8SysrFBbZ5nb2u30GZ0EKWVF\nyS/HyThonzThc1WTNkm9Pn/NbeNyY6vD5bUyaNdUBBlxE6Kljsf8+qjrQXK7R62rqOXvBvtHE5u/\nvNs4IcSbC/2CTohMULALkQkKdiEyQcEuRCYo2IXIhDn/ysVQk0OOg0y0mpjqoFXTICheGGWiNYNx\nDU9LdoNAJtvY2qa2nV5aPgGA4Sho/xSctd4gLSl5IL2V0Y+dglZIO0FmXk1aVI2DTMV+0PJqsBO0\nvApuWa0GMQZy6c6QS7pFyW3tdpBZGMjEI0tfP+MJn99J1POKoDu7EJmgYBciExTsQmSCgl2ITFCw\nC5EJCnYhMmGu0tvEgcE4LXlEcke3nXbTKu5+HUgro6AoX2/E/bAiLYUMidwFAJvbQVHJQGoaBxLg\n5jYvzNi7/Gpy+8i5jLO4wjPKJsEl4oGt1Uxnh42C87yxfp3axkFm3vFjvFBSc2Exud2C6yOSZreH\nXAKcBMVKveb3VSa9TWg/N6AmUnUkyOnOLkQmKNiFyAQFuxCZoGAXIhMU7EJkwtwTYdzIKm3wo/8R\nqUEXFaaeBNkRkyDpZhTU8NrYTie1bF7nq+NbQbsjC+qxjUnSDQD0hjyBZjBKz6MXQU2+oLVSL0hA\n2dzYorZ+I716Xgf1/8bBUnJJVvcBoGrz2m+TIn2JF6SlGIBpsUTCcMznCkN+ridBi6qarLp7GYwh\nl2nguu7sQuSCgl2ITFCwC5EJCnYhMkHBLkQmKNiFyIS9tH+6E8CfA7gN09/Zn3X3L5jZcQBfB3AX\npi2gPuzu16J9lVUTKytvS9p6/VfouEGdln/6pLUPAFRBS6B+IJ/UddBSirRrWr++QcdsbvIadMOg\nhlsvqE83DhI12gvp1kWdFm+wtdxJJ4sAwGLJWyEVQ67zjAZp/4MSdPBAN6oDqWxnM0g2IklKzSaX\nPatA8qoCxW484ck6wSnDmKSvTCbcx7pOh240h3u5s48B/Im7vwvAewH8oZm9C8BDAB5z93cCeGz2\ntxDiTcquwe7uF9z9x7PHmwCeAXA7gPsBPDJ72iMAPnhYTgoh9s9NfWc3s7sAvBvA4wBuc/cLM9Mr\nmH7MF0K8SdlzsJvZIoBvAviEu7/uS6pPvygkvyyY2RkzO2dm5/p9/j1UCHG47CnYzayBaaB/xd2/\nNdt80cxOzeynAFxKjXX3s+5+2t1Pt9v8981CiMNl12C3af2eLwN4xt0/d4PpUQAPzB4/AOA7B++e\nEOKg2EvW228A+BiAp8zsidm2TwL4DIBvmNnHAfwCwId3PVjVwNrJO5K28+e5aldP0h//h0FLnUhe\nswGXJ6qgNtnGxnpy+9UrV+iY7a0etZFkPgBAp9OltuWlZWpbWkiPW2rz/R1fWqW2lUVen86DewWt\noRfUaSsqbpsE1dX6QV24K1cvJ7ePav6VcnWVz+/acW4rg2xKD/yvJ+lxQQcwsFKJkbS5a7C7+98A\n9FX89m7jhRBvDvQLOiEyQcEuRCYo2IXIBAW7EJmgYBciE+ZacLKqmlhbS0tvr776Ah038XRWUyS9\n2TDKQOKyXDPIeNroET+cpzR1Fnm2mTmXmrqLXOJpN/k+O1UzuX2pxaW3jqXHAEA54nKSldz/RpHe\nZzTGGg1qK5r8Um23eMFJpkW9+PKLdMjWFi8g2mjw+TjeXqG2qC2Tk8KjQXcwsC5aKjgphFCwC5EL\nCnYhMkHBLkQmKNiFyAQFuxCZMHfp7eTJX0nafv63XE4aD9PvScNRkEnkUaEMPq7T4jJUTTK2uis8\na6xFeo0BwOWLPFuuF9huO8H3uXYsLdk1Cl5LYNgPuuaN+Dw2WlyGqphUFkhv4yHXmgZ9Xrjz+hYv\n+Ll+LZ31tr7O5bV2l0uANJsPwCjSyoKCmTUpjsokOYD3K/Qg8053diEyQcEuRCYo2IXIBAW7EJmg\nYBciE+a6Gl+WFVaXTyRtrQZvQdQbpldH3fkq8mh0a4kwVcVXYputdCukbpOvdLdJQggArF/mq8i9\noHZdEbQSapbp40XtnxbagS2ohddp89ZQZSM9J2wVGQA2+/w175BVdQC4fu06tb3yyqvJ7WPj18By\nJ0hoMR4yrJbc1BisxrPeUMHuEKzUM3RnFyITFOxCZIKCXYhMULALkQkKdiEyQcEuRCbsKr2Z2Z0A\n/hzTlswO4Ky7f8HMPg3g9wG8pm180t2/G+2rLEosLaRljeUuTybpbRKZIajhFskg9ZjbxmOeJNPq\npGudtUnLJQA43uUyTlHz99r1i1epDTXX3q5dTifQDDtcilxbO8mPFSTy1EHiR6OZlkVHNZ/fzW2e\n7DLqc/+Xu1y27bz97rShyee+aHFb1eT17moPritWNA7AmMzJpAgKyjnzMUhO4nv7/74A+BN3/7GZ\nLQH4kZl9b2b7vLv/hz3sQwhxxOyl19sFABdmjzfN7BkAtx+2Y0KIg+WmvrOb2V0A3g3g8dmmB83s\nSTN72MyOHbBvQogDZM/BbmaLAL4J4BPuvgHgiwDeAeAeTO/8nyXjzpjZOTM7d32Dt2UWQhwuewp2\nM2tgGuhfcfdvAYC7X3T32t0nAL4E4N7UWHc/6+6n3f30yrJu/kIcFbsGu5kZgC8DeMbdP3fD9lM3\nPO1DAJ4+ePeEEAfFXlbjfwPAxwA8ZWZPzLZ9EsBHzeweTOW4FwD8wW47KgLpbXV5jY775S+JxFNG\nvW74S5s4l4yiunYtUnOtF8gqNVflsLzK5UZjmVAAtq7ybLnBKJ051qqC+Rj1qQ1jntE3CWr5scTC\n0vj9pVVx2ah1bInaqiafx9rS53oQZD7ujPl8FA3ufx3IXoPgGmGZgNF1CgtT4pLsZTX+b5AW70JN\nXQjx5kK/oBMiExTsQmSCgl2ITFCwC5EJCnYhMmGuBScLK9AkhQ+Xlo/TcQMih3nJpYlWhxeORJBN\nNAyKOW7upCWZQY9LNc3g/XQhKG5ZVnxce4EXsWTFKBuBZOTgslBVRVIkzzp0pM9NEUhG3S5/XV7y\ncRPjPlZF+nUHdSPRq3nLq/GEH6sMikDWUagRH6Msuqpk+1P7JyGyR8EuRCYo2IXIBAW7EJmgYBci\nExTsQmTCXKU3mKFgvcgWeFYT62sVZag1O/yljSZcasKEa2+9/k5ye+l8TBXIJ8M2zyjrBllqzS4f\nV7KCjs7namN7ndp2+lvU1g56xHUX0kUgm63A9yaX3hoVH+clv2eNJunstp0Bl9c2tvlrnjS4vNYq\n+HyMg+ugINd3VPw0kjD5cYQQWaBgFyITFOxCZIKCXYhMULALkQkKdiEyYb7SGwBWo7DZDCSZMp0d\n1htwCW005BlxdfAeNxrzQoQ9kvU2GaaLPAJhgh3ceY+yotuhtm4wV0ayngoLMrKCYohXN65TW4fI\nWgBQLqTPmZMMLwBoVEEWXcnlzUhK7ZNimus7XF57NXjNZZv3euvSTDSgCLL2rE77P+gHch2Rez3I\nytOdXYhMULALkQkKdiEyQcEuRCYo2IXIhF1X482sDeD7AFqz5/+Fu3/KzO4G8DUAawB+BOBj7j4M\nd+YASEubVoO70iIJEtc3eDJDb4evZDbaPOFiEnTc6ZPV/60tvrJbRi2NgkSYKqxPFygXZJ9Fg79m\na/MXXQQtiMou721VLKfVhHKBr2ZbxX3sBavM/aCVU5+0f1of8WvnyvY2tbWijkxN/trabX4+a7Ia\nPx4G92I+VZS93NkHAH7L3X8d0/bM95nZewH8KYDPu/s/AnANwMdv/vBCiHmxa7D7lNduXY3ZPwfw\nWwD+Yrb9EQAfPBQPhRAHwl77s5ezDq6XAHwPwHMA1t39tc8fLwG4/XBcFEIcBHsKdnev3f0eAHcA\nuBfAP97rAczsjJmdM7Nz69ev3KKbQoj9clOr8e6+DuCvAfxTAKtmf1dq/w4A58mYs+5+2t1Pr67w\nHuxCiMNl12A3s5Nmtjp73AHwOwCewTTo/8XsaQ8A+M5hOSmE2D97SYQ5BeARMysxfXP4hrv/dzP7\nGYCvmdm/A/C/AXx59105QNoClUGiQKuZdrMIskz6PS6f1EHNOCuDZAxLvzeOAzlmq8clnu0BVyrb\nQbJLm9WZA9Agtc7Cd/WonFkgU3qLy0neSp8zJ+cSAEYFd2QU1GPbCRJhtonEtkXkLgDYqYMeYOMg\nISfYZxGI0ix5pTQu5d1KDbpdg93dnwTw7sT25zH9/i6E+HuAfkEnRCYo2IXIBAW7EJmgYBciExTs\nQmSCedAW6MAPZvYqgF/M/jwB4PLcDs6RH69Hfryev29+/Kq7n0wZ5hrsrzuw2Tl3P30kB5cf8iND\nP/QxXohMULALkQlHGexnj/DYNyI/Xo/8eD3/YPw4su/sQoj5oo/xQmTCkQS7md1nZv/XzJ41s4eO\nwoeZHy+Y2VNm9oSZnZvjcR82s0tm9vQN246b2ffM7Oez/48dkR+fNrPzszl5wsw+MAc/7jSzvzaz\nn5nZT83sj2bb5zongR9znRMza5vZD8zsJzM//u1s+91m9vgsbr5uZjdXdtLd5/oPQIlpWau3Y1oj\n8ycA3jVvP2a+vADgxBEc9zcBvAfA0zds+/cAHpo9fgjAnx6RH58G8K/mPB+nALxn9ngJwN8CeNe8\n5yTwY65zgmnS8eLscQPA4wDeC+AbAD4y2/6fAPzLm9nvUdzZ7wXwrLs/79PS018DcP8R+HFkuPv3\nAVx9w+b7MS3cCcypgCfxY+64+wV3//Hs8SamxVFux5znJPBjrviUAy/yehTBfjuAF2/4+yiLVTqA\nvzSzH5nZmSPy4TVuc/cLs8evALjtCH150MyenH3MP/SvEzdiZndhWj/hcRzhnLzBD2DOc3IYRV5z\nX6B7n7u/B8A/B/CHZvabR+0QMH1nB21ufeh8EcA7MO0RcAHAZ+d1YDNbBPBNAJ9w940bbfOck4Qf\nc58T30eRV8ZRBPt5AHfe8DctVnnYuPv52f+XAHwbR1t556KZnQKA2f+XjsIJd784u9AmAL6EOc2J\nmTUwDbCvuPu3ZpvnPicpP45qTmbHvukir4yjCPYfAnjnbGWxCeAjAB6dtxNm1jWzpdceA/hdAE/H\now6VRzEt3AkcYQHP14JrxocwhzkxM8O0huEz7v65G0xznRPmx7zn5NCKvM5rhfENq40fwHSl8zkA\n//qIfHg7pkrATwD8dJ5+APgqph8HR5h+9/o4pj3zHgPwcwB/BeD4EfnxXwA8BeBJTIPt1Bz8eB+m\nH9GfBPDE7N8H5j0ngR9znRMA/wTTIq5PYvrG8m9uuGZ/AOBZAP8NQOtm9qtf0AmRCbkv0AmRDQp2\nITJBwS5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhM+H89r64NINFIYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15kfMz8qLqx3",
        "colab_type": "text"
      },
      "source": [
        "**Parameter Inizialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a14TMPgLupi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight initialization\n",
        "def init_weights(m): \n",
        "\tif type(m)==nn.Linear:\n",
        "\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
        "\t\tm.bias.data.fill_(0.01)\n",
        "\t\n",
        "# Building Generator \n",
        "generator = Generator(N_CLASSES, LAT_DIM, BATCH_SIZE, dataset_name, img_shape)\n",
        "# gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lrate, betas=(beta, beta1))\n",
        "gen_optimizer = optim.SGD(generator.parameters(), lr=LR, momentum=0.9, weight_decay=5e-5)\n",
        "generator.load_state_dict(torch.load(\"%sgenerator.pth\" % PATH_g))\n",
        "\n",
        "# Building Discriminator  \n",
        "discriminator = Discriminator(N_CLASSES, LAT_DIM, BATCH_SIZE, img_shape, dataset_name)\n",
        "discriminator.load_state_dict(torch.load(\"%sdiscriminator.pth\" % PATH_d))\n",
        "#discriminator.apply(init_weights)\n",
        "# d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lrate, betas=(beta, beta1))\n",
        "d_optimizer = optim.SGD(discriminator.parameters(), lr=LR, momentum=0.7, weight_decay=5e-5)\n",
        "\n",
        "a_loss = torch.nn.BCELoss() # Loss functions \n",
        "\n",
        "# Labels \n",
        "real_label = 0.9\n",
        "fake_label = 0.0\n",
        "\n",
        "FT = torch.LongTensor\n",
        "FT_a = torch.FloatTensor\n",
        "\n",
        "if cuda: \n",
        "\tgenerator.cuda()\n",
        "\tdiscriminator.cuda()\n",
        "\ta_loss.cuda()\n",
        "\tFT = torch.cuda.LongTensor\n",
        "\tFT_a = torch.cuda.FloatTensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV8TbBv1L2Gv",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-i06GPL4oW",
        "colab_type": "code",
        "outputId": "228fbeb2-ff88-4ff2-e591-422f79bf777d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " fixed_noise=Variable(FT_a(np.random.normal(0, 1, (64, LAT_DIM))))\n",
        " imgs, labels=next(iter(dataloader))\n",
        " fixed_labels= Variable(labels[0:64].type(FT_a))\n",
        "\n",
        " for epoch in range(1062,EPOCHS): \n",
        "\tfor i, (imgs, labels) in enumerate(dataloader): \n",
        "\t\tbatch_size = imgs.shape[0]\n",
        "\t\t\n",
        "\t\t# convert img, labels into proper form \n",
        "\t\timgs = Variable(imgs.type(FT_a))\n",
        "\t\tlabels = Variable(labels.type(FT_a))\n",
        "\t\n",
        "\t\t# creating real and fake tensors of labels \n",
        "\t\treall = Variable(FT_a(batch_size, 1).fill_(real_label)) \t\t# label delle immagini reali per il discriminatore\n",
        "\t\tf_label = Variable(FT_a(batch_size, 1).fill_(fake_label)) \t# label delle immagini fake per il discriminatore\n",
        "\n",
        "\t\t# initializing gradient\n",
        "\t\tgen_optimizer.zero_grad() \n",
        "\t\td_optimizer.zero_grad()\n",
        "\n",
        "\t\t#### TRAINING DISCRIMINATOR ####\n",
        "\t\tnoise = Variable(FT_a(np.random.normal(0, 1, (batch_size, LAT_DIM)))) # noise ha dimensione imgs.shape[0]xlatentdim imgs dovrebbero essere 64 foto (dimensione di un batch) x 100 pixel ogni foto?\n",
        "\t\t## Prima generiamo le immagini false tramite il generatore\n",
        "\n",
        "\t\tgen_labels = Variable(FT(np.random.randint(0, N_CLASSES, batch_size))) # sceglie dei numeri a caso da generare per le 64 immagini\n",
        "\t\tif dataset_name == 'lfwcrop':\n",
        "\t\t\t gen_labels =labels\n",
        "\t\tgen_imgs = generator(noise, gen_labels, batch_size) # chiama il generatore passandogli il noise e le labels delle immagini che si aspetta vengano generati\n",
        "\n",
        "\t\t## Facciamo il training con le sole immagini vere e poi facciamo backward\n",
        "\t\td_optimizer.zero_grad()\n",
        "\n",
        "\t\t# Loss for real images and labels \n",
        "\t\tvalidity_real = discriminator(imgs, labels, batch_size) \t# fai il training usando le immagini vere\n",
        "\t\td_real_loss = a_loss(validity_real, reall) \t\t# quante immagini vere sono state riconosciute come tali?\n",
        "\n",
        "\t\td_real_loss.backward()\n",
        "\t\n",
        "\t\t## Facciamo il training con le sole immagini false e facciamo backward\n",
        "\t\t# Loss for fake images and labels \n",
        "\t\tvalidity_fake = discriminator(gen_imgs.detach(), gen_labels, batch_size) # quante immagini false ha riconosciuto?\n",
        "\t\td_fake_loss = a_loss(validity_fake, f_label)\n",
        "\t\n",
        "\t\td_fake_loss.backward()\n",
        "\n",
        "\t\t# Total discriminator loss \n",
        "\t\td_loss = (d_fake_loss + d_real_loss) # fai una media dell'errore su vere e su false e usa questo nel backward\n",
        "\n",
        "\t\t# calculates discriminator gradients\n",
        "\t\t#d_loss.backward()\n",
        "\t\td_optimizer.step()\n",
        "\t\t\t\n",
        "\t\t#### TRAINING GENERATOR ####\n",
        "\t\t# Ability for discriminator to discern the real v generated images \n",
        "\t\tvalidity = discriminator(gen_imgs, gen_labels, batch_size)   # fai il train del discriminator sulle immagini false\n",
        "\t\t\n",
        "\t\t# Generative loss function \n",
        "\t\tg_loss = a_loss(validity, reall)  # loss in base a quante immagini il discriminatore ha visto come reali (validity) \n",
        "\n",
        "\t\t# Gradients \n",
        "\t\tg_loss.backward()\n",
        "\t\tgen_optimizer.step()\n",
        "\n",
        "\tif epoch % 50 == 0: \n",
        "\t\tfake = generator(fixed_noise, fixed_labels, 64)\n",
        "\t\tvutils.save_image(fake.detach(), '%sfake_samples_epoch_%03d.png' % (outputdir, epoch), normalize=True)\n",
        "\t\n",
        "\tprint(\"[Epoch: %d/%d]\" \"[D loss: %f]\" \"[G loss: %f]\" % (epoch+1, EPOCHS, d_loss.item(), g_loss.item()))\n",
        "\t\n",
        "\t# checkpoints \n",
        "\ttorch.save(generator.state_dict(), '%s/generator.pth' % (outputdir))\n",
        "\ttorch.save(discriminator.state_dict(), '%s/discriminator.pth' % (outputdir))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1063/2000][D loss: 1.203470][G loss: 1.236098]\n",
            "[Epoch: 1064/2000][D loss: 1.060650][G loss: 1.451243]\n",
            "[Epoch: 1065/2000][D loss: 1.071026][G loss: 1.266758]\n",
            "[Epoch: 1066/2000][D loss: 1.067706][G loss: 1.331660]\n",
            "[Epoch: 1067/2000][D loss: 0.994223][G loss: 1.605243]\n",
            "[Epoch: 1068/2000][D loss: 1.067486][G loss: 1.270620]\n",
            "[Epoch: 1069/2000][D loss: 1.109545][G loss: 1.285004]\n",
            "[Epoch: 1070/2000][D loss: 1.006118][G loss: 1.323966]\n",
            "[Epoch: 1071/2000][D loss: 1.008310][G loss: 1.487800]\n",
            "[Epoch: 1072/2000][D loss: 1.090077][G loss: 1.541697]\n",
            "[Epoch: 1073/2000][D loss: 1.068514][G loss: 1.296158]\n",
            "[Epoch: 1074/2000][D loss: 1.032225][G loss: 1.278126]\n",
            "[Epoch: 1075/2000][D loss: 1.138337][G loss: 1.376342]\n",
            "[Epoch: 1076/2000][D loss: 0.996176][G loss: 1.346431]\n",
            "[Epoch: 1077/2000][D loss: 1.192698][G loss: 1.236304]\n",
            "[Epoch: 1078/2000][D loss: 1.090604][G loss: 1.452981]\n",
            "[Epoch: 1079/2000][D loss: 0.966226][G loss: 1.405291]\n",
            "[Epoch: 1080/2000][D loss: 1.024380][G loss: 1.356089]\n",
            "[Epoch: 1081/2000][D loss: 1.095134][G loss: 1.553470]\n",
            "[Epoch: 1082/2000][D loss: 1.092180][G loss: 1.295398]\n",
            "[Epoch: 1083/2000][D loss: 1.083680][G loss: 1.321834]\n",
            "[Epoch: 1084/2000][D loss: 1.047945][G loss: 1.322927]\n",
            "[Epoch: 1085/2000][D loss: 0.975379][G loss: 1.542568]\n",
            "[Epoch: 1086/2000][D loss: 1.011876][G loss: 1.301518]\n",
            "[Epoch: 1087/2000][D loss: 1.085761][G loss: 1.245464]\n",
            "[Epoch: 1088/2000][D loss: 0.995483][G loss: 1.229780]\n",
            "[Epoch: 1089/2000][D loss: 1.044831][G loss: 1.336248]\n",
            "[Epoch: 1090/2000][D loss: 1.005951][G loss: 1.370904]\n",
            "[Epoch: 1091/2000][D loss: 1.002434][G loss: 1.246930]\n",
            "[Epoch: 1092/2000][D loss: 1.171467][G loss: 1.214908]\n",
            "[Epoch: 1093/2000][D loss: 1.022874][G loss: 1.488465]\n",
            "[Epoch: 1094/2000][D loss: 1.126696][G loss: 1.405986]\n",
            "[Epoch: 1095/2000][D loss: 1.199527][G loss: 1.226030]\n",
            "[Epoch: 1096/2000][D loss: 1.021729][G loss: 1.375232]\n",
            "[Epoch: 1097/2000][D loss: 1.178909][G loss: 1.410500]\n",
            "[Epoch: 1098/2000][D loss: 1.027344][G loss: 1.198802]\n",
            "[Epoch: 1099/2000][D loss: 1.017162][G loss: 1.400698]\n",
            "[Epoch: 1100/2000][D loss: 1.195989][G loss: 1.211514]\n",
            "[Epoch: 1101/2000][D loss: 1.043919][G loss: 1.365059]\n",
            "[Epoch: 1102/2000][D loss: 1.045666][G loss: 1.168115]\n",
            "[Epoch: 1103/2000][D loss: 1.017395][G loss: 1.349656]\n",
            "[Epoch: 1104/2000][D loss: 1.015429][G loss: 1.362044]\n",
            "[Epoch: 1105/2000][D loss: 1.163747][G loss: 1.345684]\n",
            "[Epoch: 1106/2000][D loss: 1.190274][G loss: 1.158902]\n",
            "[Epoch: 1107/2000][D loss: 1.113515][G loss: 1.228096]\n",
            "[Epoch: 1108/2000][D loss: 1.102061][G loss: 1.332359]\n",
            "[Epoch: 1109/2000][D loss: 1.012821][G loss: 1.384723]\n",
            "[Epoch: 1110/2000][D loss: 1.124851][G loss: 1.364434]\n",
            "[Epoch: 1111/2000][D loss: 1.001323][G loss: 1.364669]\n",
            "[Epoch: 1112/2000][D loss: 1.118446][G loss: 1.258041]\n",
            "[Epoch: 1113/2000][D loss: 1.149246][G loss: 1.237612]\n",
            "[Epoch: 1114/2000][D loss: 1.093134][G loss: 1.293817]\n",
            "[Epoch: 1115/2000][D loss: 1.172980][G loss: 1.262539]\n",
            "[Epoch: 1116/2000][D loss: 1.063624][G loss: 1.329694]\n",
            "[Epoch: 1117/2000][D loss: 0.971389][G loss: 1.479915]\n",
            "[Epoch: 1118/2000][D loss: 1.090311][G loss: 1.163001]\n",
            "[Epoch: 1119/2000][D loss: 1.128887][G loss: 1.222529]\n",
            "[Epoch: 1120/2000][D loss: 1.135916][G loss: 1.463928]\n",
            "[Epoch: 1121/2000][D loss: 1.171734][G loss: 1.213762]\n",
            "[Epoch: 1122/2000][D loss: 1.141984][G loss: 1.196684]\n",
            "[Epoch: 1123/2000][D loss: 1.036626][G loss: 1.351384]\n",
            "[Epoch: 1124/2000][D loss: 1.032943][G loss: 1.485198]\n",
            "[Epoch: 1125/2000][D loss: 1.090461][G loss: 1.419072]\n",
            "[Epoch: 1126/2000][D loss: 1.095502][G loss: 1.277423]\n",
            "[Epoch: 1127/2000][D loss: 1.019456][G loss: 1.427438]\n",
            "[Epoch: 1128/2000][D loss: 1.094361][G loss: 1.260824]\n",
            "[Epoch: 1129/2000][D loss: 1.063859][G loss: 1.430927]\n",
            "[Epoch: 1130/2000][D loss: 1.041663][G loss: 1.274958]\n",
            "[Epoch: 1131/2000][D loss: 1.059205][G loss: 1.499992]\n",
            "[Epoch: 1132/2000][D loss: 1.003779][G loss: 1.410455]\n",
            "[Epoch: 1133/2000][D loss: 1.024855][G loss: 1.344849]\n",
            "[Epoch: 1134/2000][D loss: 1.051009][G loss: 1.407003]\n",
            "[Epoch: 1135/2000][D loss: 1.130853][G loss: 1.176663]\n",
            "[Epoch: 1136/2000][D loss: 1.034575][G loss: 1.369139]\n",
            "[Epoch: 1137/2000][D loss: 1.110261][G loss: 1.554865]\n",
            "[Epoch: 1138/2000][D loss: 1.044972][G loss: 1.438723]\n",
            "[Epoch: 1139/2000][D loss: 1.057941][G loss: 1.360384]\n",
            "[Epoch: 1140/2000][D loss: 1.001985][G loss: 1.394866]\n",
            "[Epoch: 1141/2000][D loss: 1.064630][G loss: 1.406376]\n",
            "[Epoch: 1142/2000][D loss: 1.214916][G loss: 1.200517]\n",
            "[Epoch: 1143/2000][D loss: 0.936073][G loss: 1.303578]\n",
            "[Epoch: 1144/2000][D loss: 1.091297][G loss: 1.409859]\n",
            "[Epoch: 1145/2000][D loss: 1.110762][G loss: 1.309816]\n",
            "[Epoch: 1146/2000][D loss: 0.964258][G loss: 1.429239]\n",
            "[Epoch: 1147/2000][D loss: 1.129938][G loss: 1.296976]\n",
            "[Epoch: 1148/2000][D loss: 0.918939][G loss: 1.354878]\n",
            "[Epoch: 1149/2000][D loss: 1.089227][G loss: 1.400407]\n",
            "[Epoch: 1150/2000][D loss: 1.115464][G loss: 1.251300]\n",
            "[Epoch: 1151/2000][D loss: 1.055477][G loss: 1.393648]\n",
            "[Epoch: 1152/2000][D loss: 1.089859][G loss: 1.204856]\n",
            "[Epoch: 1153/2000][D loss: 1.186946][G loss: 1.284204]\n",
            "[Epoch: 1154/2000][D loss: 1.269345][G loss: 1.279559]\n",
            "[Epoch: 1155/2000][D loss: 1.219808][G loss: 1.118405]\n",
            "[Epoch: 1156/2000][D loss: 1.109570][G loss: 1.282744]\n",
            "[Epoch: 1157/2000][D loss: 1.031244][G loss: 1.398275]\n",
            "[Epoch: 1158/2000][D loss: 1.101837][G loss: 1.338786]\n",
            "[Epoch: 1159/2000][D loss: 1.147051][G loss: 1.347559]\n",
            "[Epoch: 1160/2000][D loss: 1.181499][G loss: 1.282055]\n",
            "[Epoch: 1161/2000][D loss: 1.005779][G loss: 1.203475]\n",
            "[Epoch: 1162/2000][D loss: 0.944934][G loss: 1.434416]\n",
            "[Epoch: 1163/2000][D loss: 1.033282][G loss: 1.191672]\n",
            "[Epoch: 1164/2000][D loss: 1.112447][G loss: 1.229980]\n",
            "[Epoch: 1165/2000][D loss: 1.098502][G loss: 1.326404]\n",
            "[Epoch: 1166/2000][D loss: 1.008880][G loss: 1.355887]\n",
            "[Epoch: 1167/2000][D loss: 1.053555][G loss: 1.356136]\n",
            "[Epoch: 1168/2000][D loss: 0.949977][G loss: 1.252110]\n",
            "[Epoch: 1169/2000][D loss: 1.104867][G loss: 1.311565]\n",
            "[Epoch: 1170/2000][D loss: 0.996576][G loss: 1.329771]\n",
            "[Epoch: 1171/2000][D loss: 0.980046][G loss: 1.413388]\n",
            "[Epoch: 1172/2000][D loss: 1.102797][G loss: 1.097805]\n",
            "[Epoch: 1173/2000][D loss: 1.071178][G loss: 1.195474]\n",
            "[Epoch: 1174/2000][D loss: 1.053653][G loss: 1.423138]\n",
            "[Epoch: 1175/2000][D loss: 1.023675][G loss: 1.305843]\n",
            "[Epoch: 1176/2000][D loss: 1.056219][G loss: 1.308896]\n",
            "[Epoch: 1177/2000][D loss: 1.045464][G loss: 1.144699]\n",
            "[Epoch: 1178/2000][D loss: 1.094045][G loss: 1.322416]\n",
            "[Epoch: 1179/2000][D loss: 1.049596][G loss: 1.310610]\n",
            "[Epoch: 1180/2000][D loss: 1.112842][G loss: 1.389871]\n",
            "[Epoch: 1181/2000][D loss: 1.117646][G loss: 1.266529]\n",
            "[Epoch: 1182/2000][D loss: 1.215575][G loss: 1.207094]\n",
            "[Epoch: 1183/2000][D loss: 0.989490][G loss: 1.425916]\n",
            "[Epoch: 1184/2000][D loss: 1.147075][G loss: 1.332558]\n",
            "[Epoch: 1185/2000][D loss: 1.057233][G loss: 1.265238]\n",
            "[Epoch: 1186/2000][D loss: 1.087265][G loss: 1.330664]\n",
            "[Epoch: 1187/2000][D loss: 1.151029][G loss: 1.262837]\n",
            "[Epoch: 1188/2000][D loss: 1.136962][G loss: 1.297491]\n",
            "[Epoch: 1189/2000][D loss: 1.070584][G loss: 1.242895]\n",
            "[Epoch: 1190/2000][D loss: 1.029631][G loss: 1.395895]\n",
            "[Epoch: 1191/2000][D loss: 1.034913][G loss: 1.321532]\n",
            "[Epoch: 1192/2000][D loss: 1.014063][G loss: 1.322881]\n",
            "[Epoch: 1193/2000][D loss: 1.017774][G loss: 1.339255]\n",
            "[Epoch: 1194/2000][D loss: 1.083336][G loss: 1.302151]\n",
            "[Epoch: 1195/2000][D loss: 1.062404][G loss: 1.239166]\n",
            "[Epoch: 1196/2000][D loss: 1.189193][G loss: 1.302348]\n",
            "[Epoch: 1197/2000][D loss: 1.069407][G loss: 1.261695]\n",
            "[Epoch: 1198/2000][D loss: 1.070128][G loss: 1.196230]\n",
            "[Epoch: 1199/2000][D loss: 1.145717][G loss: 1.406268]\n",
            "[Epoch: 1200/2000][D loss: 1.108302][G loss: 1.224526]\n",
            "[Epoch: 1201/2000][D loss: 1.153363][G loss: 1.284398]\n",
            "[Epoch: 1202/2000][D loss: 1.149582][G loss: 1.324754]\n",
            "[Epoch: 1203/2000][D loss: 1.036004][G loss: 1.366712]\n",
            "[Epoch: 1204/2000][D loss: 1.127457][G loss: 1.152184]\n",
            "[Epoch: 1205/2000][D loss: 1.027058][G loss: 1.320162]\n",
            "[Epoch: 1206/2000][D loss: 1.070378][G loss: 1.237902]\n",
            "[Epoch: 1207/2000][D loss: 0.994860][G loss: 1.277624]\n",
            "[Epoch: 1208/2000][D loss: 0.983008][G loss: 1.224759]\n",
            "[Epoch: 1209/2000][D loss: 1.007793][G loss: 1.422761]\n",
            "[Epoch: 1210/2000][D loss: 1.104150][G loss: 1.330793]\n",
            "[Epoch: 1211/2000][D loss: 1.048652][G loss: 1.403930]\n",
            "[Epoch: 1212/2000][D loss: 1.172817][G loss: 1.266892]\n",
            "[Epoch: 1213/2000][D loss: 1.103893][G loss: 1.382238]\n",
            "[Epoch: 1214/2000][D loss: 1.035087][G loss: 1.379579]\n",
            "[Epoch: 1215/2000][D loss: 1.077325][G loss: 1.349618]\n",
            "[Epoch: 1216/2000][D loss: 1.111056][G loss: 1.367381]\n",
            "[Epoch: 1217/2000][D loss: 1.141316][G loss: 1.144192]\n",
            "[Epoch: 1218/2000][D loss: 1.027614][G loss: 1.274017]\n",
            "[Epoch: 1219/2000][D loss: 1.021765][G loss: 1.279232]\n",
            "[Epoch: 1220/2000][D loss: 1.024997][G loss: 1.380045]\n",
            "[Epoch: 1221/2000][D loss: 1.058175][G loss: 1.424790]\n",
            "[Epoch: 1222/2000][D loss: 1.036561][G loss: 1.347824]\n",
            "[Epoch: 1223/2000][D loss: 0.984329][G loss: 1.644522]\n",
            "[Epoch: 1224/2000][D loss: 1.005069][G loss: 1.143739]\n",
            "[Epoch: 1225/2000][D loss: 1.053433][G loss: 1.493031]\n",
            "[Epoch: 1226/2000][D loss: 1.025867][G loss: 1.256046]\n",
            "[Epoch: 1227/2000][D loss: 0.952583][G loss: 1.312832]\n",
            "[Epoch: 1228/2000][D loss: 1.168228][G loss: 1.371830]\n",
            "[Epoch: 1229/2000][D loss: 0.967104][G loss: 1.294237]\n",
            "[Epoch: 1230/2000][D loss: 1.019811][G loss: 1.255378]\n",
            "[Epoch: 1231/2000][D loss: 1.180705][G loss: 1.297870]\n",
            "[Epoch: 1232/2000][D loss: 1.070063][G loss: 1.408407]\n",
            "[Epoch: 1233/2000][D loss: 1.121363][G loss: 1.194554]\n",
            "[Epoch: 1234/2000][D loss: 0.998357][G loss: 1.410293]\n",
            "[Epoch: 1235/2000][D loss: 1.021925][G loss: 1.445236]\n",
            "[Epoch: 1236/2000][D loss: 1.050684][G loss: 1.332112]\n",
            "[Epoch: 1237/2000][D loss: 0.968248][G loss: 1.457118]\n",
            "[Epoch: 1238/2000][D loss: 0.995127][G loss: 1.289086]\n",
            "[Epoch: 1239/2000][D loss: 1.115071][G loss: 1.415519]\n",
            "[Epoch: 1240/2000][D loss: 1.082239][G loss: 1.358618]\n",
            "[Epoch: 1241/2000][D loss: 1.166375][G loss: 1.390067]\n",
            "[Epoch: 1242/2000][D loss: 1.102994][G loss: 1.251332]\n",
            "[Epoch: 1243/2000][D loss: 1.024700][G loss: 1.409036]\n",
            "[Epoch: 1244/2000][D loss: 1.215875][G loss: 1.324168]\n",
            "[Epoch: 1245/2000][D loss: 1.153569][G loss: 1.285788]\n",
            "[Epoch: 1246/2000][D loss: 1.001150][G loss: 1.466415]\n",
            "[Epoch: 1247/2000][D loss: 1.025525][G loss: 1.376958]\n",
            "[Epoch: 1248/2000][D loss: 1.042023][G loss: 1.301676]\n",
            "[Epoch: 1249/2000][D loss: 1.048878][G loss: 1.453130]\n",
            "[Epoch: 1250/2000][D loss: 1.142178][G loss: 1.196404]\n",
            "[Epoch: 1251/2000][D loss: 1.025919][G loss: 1.262087]\n",
            "[Epoch: 1252/2000][D loss: 1.183725][G loss: 1.053284]\n",
            "[Epoch: 1253/2000][D loss: 1.024305][G loss: 1.330580]\n",
            "[Epoch: 1254/2000][D loss: 1.078739][G loss: 1.222159]\n",
            "[Epoch: 1255/2000][D loss: 0.977382][G loss: 1.283442]\n",
            "[Epoch: 1256/2000][D loss: 1.150658][G loss: 1.504200]\n",
            "[Epoch: 1257/2000][D loss: 1.000641][G loss: 1.527984]\n",
            "[Epoch: 1258/2000][D loss: 1.112921][G loss: 1.411744]\n",
            "[Epoch: 1259/2000][D loss: 1.016201][G loss: 1.362524]\n",
            "[Epoch: 1260/2000][D loss: 0.956869][G loss: 1.328600]\n",
            "[Epoch: 1261/2000][D loss: 1.061767][G loss: 1.257609]\n",
            "[Epoch: 1262/2000][D loss: 1.049182][G loss: 1.274479]\n",
            "[Epoch: 1263/2000][D loss: 1.006495][G loss: 1.482776]\n",
            "[Epoch: 1264/2000][D loss: 1.010186][G loss: 1.336826]\n",
            "[Epoch: 1265/2000][D loss: 0.936130][G loss: 1.179983]\n",
            "[Epoch: 1266/2000][D loss: 0.965542][G loss: 1.319276]\n",
            "[Epoch: 1267/2000][D loss: 1.084533][G loss: 1.184857]\n",
            "[Epoch: 1268/2000][D loss: 1.002772][G loss: 1.236801]\n",
            "[Epoch: 1269/2000][D loss: 1.080733][G loss: 1.264095]\n",
            "[Epoch: 1270/2000][D loss: 1.071123][G loss: 1.230888]\n",
            "[Epoch: 1271/2000][D loss: 1.002885][G loss: 1.336448]\n",
            "[Epoch: 1272/2000][D loss: 1.099063][G loss: 1.406978]\n",
            "[Epoch: 1273/2000][D loss: 0.957666][G loss: 1.420939]\n",
            "[Epoch: 1274/2000][D loss: 1.168155][G loss: 1.259161]\n",
            "[Epoch: 1275/2000][D loss: 1.084364][G loss: 1.335482]\n",
            "[Epoch: 1276/2000][D loss: 1.005637][G loss: 1.289781]\n",
            "[Epoch: 1277/2000][D loss: 1.146351][G loss: 1.424762]\n",
            "[Epoch: 1278/2000][D loss: 1.057762][G loss: 1.492233]\n",
            "[Epoch: 1279/2000][D loss: 0.969167][G loss: 1.341991]\n",
            "[Epoch: 1280/2000][D loss: 1.105891][G loss: 1.449085]\n",
            "[Epoch: 1281/2000][D loss: 0.932441][G loss: 1.217493]\n",
            "[Epoch: 1282/2000][D loss: 1.152720][G loss: 1.485359]\n",
            "[Epoch: 1283/2000][D loss: 1.037338][G loss: 1.435342]\n",
            "[Epoch: 1284/2000][D loss: 1.005929][G loss: 1.405486]\n",
            "[Epoch: 1285/2000][D loss: 1.008337][G loss: 1.513143]\n",
            "[Epoch: 1286/2000][D loss: 1.030107][G loss: 1.369451]\n",
            "[Epoch: 1287/2000][D loss: 1.175622][G loss: 1.203974]\n",
            "[Epoch: 1288/2000][D loss: 1.106278][G loss: 1.339240]\n",
            "[Epoch: 1289/2000][D loss: 0.946739][G loss: 1.353797]\n",
            "[Epoch: 1290/2000][D loss: 1.175738][G loss: 1.274940]\n",
            "[Epoch: 1291/2000][D loss: 1.049136][G loss: 1.264805]\n",
            "[Epoch: 1292/2000][D loss: 1.041685][G loss: 1.317550]\n",
            "[Epoch: 1293/2000][D loss: 0.944624][G loss: 1.449248]\n",
            "[Epoch: 1294/2000][D loss: 1.005965][G loss: 1.523433]\n",
            "[Epoch: 1295/2000][D loss: 1.015733][G loss: 1.315270]\n",
            "[Epoch: 1296/2000][D loss: 0.975367][G loss: 1.335128]\n",
            "[Epoch: 1297/2000][D loss: 1.031213][G loss: 1.389794]\n",
            "[Epoch: 1298/2000][D loss: 1.099967][G loss: 1.318468]\n",
            "[Epoch: 1299/2000][D loss: 1.019499][G loss: 1.388345]\n",
            "[Epoch: 1300/2000][D loss: 1.085986][G loss: 1.387605]\n",
            "[Epoch: 1301/2000][D loss: 0.974559][G loss: 1.443143]\n",
            "[Epoch: 1302/2000][D loss: 1.002054][G loss: 1.417859]\n",
            "[Epoch: 1303/2000][D loss: 1.124529][G loss: 1.365676]\n",
            "[Epoch: 1304/2000][D loss: 1.160599][G loss: 1.306222]\n",
            "[Epoch: 1305/2000][D loss: 1.181740][G loss: 1.258667]\n",
            "[Epoch: 1306/2000][D loss: 0.974898][G loss: 1.415098]\n",
            "[Epoch: 1307/2000][D loss: 1.004940][G loss: 1.352722]\n",
            "[Epoch: 1308/2000][D loss: 1.046942][G loss: 1.326812]\n",
            "[Epoch: 1309/2000][D loss: 1.094542][G loss: 1.367780]\n",
            "[Epoch: 1310/2000][D loss: 0.960118][G loss: 1.415881]\n",
            "[Epoch: 1311/2000][D loss: 1.189654][G loss: 1.189803]\n",
            "[Epoch: 1312/2000][D loss: 1.047208][G loss: 1.439536]\n",
            "[Epoch: 1313/2000][D loss: 1.071366][G loss: 1.344739]\n",
            "[Epoch: 1314/2000][D loss: 1.114374][G loss: 1.352753]\n",
            "[Epoch: 1315/2000][D loss: 1.033951][G loss: 1.337390]\n",
            "[Epoch: 1316/2000][D loss: 1.039578][G loss: 1.201611]\n",
            "[Epoch: 1317/2000][D loss: 1.139457][G loss: 1.434561]\n",
            "[Epoch: 1318/2000][D loss: 0.952578][G loss: 1.387984]\n",
            "[Epoch: 1319/2000][D loss: 1.023648][G loss: 1.262257]\n",
            "[Epoch: 1320/2000][D loss: 1.063699][G loss: 1.108556]\n",
            "[Epoch: 1321/2000][D loss: 1.155187][G loss: 1.244937]\n",
            "[Epoch: 1322/2000][D loss: 1.098654][G loss: 1.378151]\n",
            "[Epoch: 1323/2000][D loss: 1.095899][G loss: 1.278121]\n",
            "[Epoch: 1324/2000][D loss: 1.149740][G loss: 1.466224]\n",
            "[Epoch: 1325/2000][D loss: 1.042976][G loss: 1.339143]\n",
            "[Epoch: 1326/2000][D loss: 0.974775][G loss: 1.387207]\n",
            "[Epoch: 1327/2000][D loss: 0.975872][G loss: 1.301492]\n",
            "[Epoch: 1328/2000][D loss: 1.085996][G loss: 1.283823]\n",
            "[Epoch: 1329/2000][D loss: 1.043673][G loss: 1.333347]\n",
            "[Epoch: 1330/2000][D loss: 1.023389][G loss: 1.352429]\n",
            "[Epoch: 1331/2000][D loss: 1.093971][G loss: 1.376130]\n",
            "[Epoch: 1332/2000][D loss: 0.982431][G loss: 1.203196]\n",
            "[Epoch: 1333/2000][D loss: 0.986798][G loss: 1.227213]\n",
            "[Epoch: 1334/2000][D loss: 1.038559][G loss: 1.364011]\n",
            "[Epoch: 1335/2000][D loss: 0.983027][G loss: 1.488529]\n",
            "[Epoch: 1336/2000][D loss: 0.997973][G loss: 1.262672]\n",
            "[Epoch: 1337/2000][D loss: 1.102759][G loss: 1.296951]\n",
            "[Epoch: 1338/2000][D loss: 1.170251][G loss: 1.333847]\n",
            "[Epoch: 1339/2000][D loss: 1.064667][G loss: 1.353494]\n",
            "[Epoch: 1340/2000][D loss: 1.003747][G loss: 1.282972]\n",
            "[Epoch: 1341/2000][D loss: 1.189247][G loss: 1.444369]\n",
            "[Epoch: 1342/2000][D loss: 1.091048][G loss: 1.068333]\n",
            "[Epoch: 1343/2000][D loss: 1.074146][G loss: 1.379926]\n",
            "[Epoch: 1344/2000][D loss: 1.087176][G loss: 1.318537]\n",
            "[Epoch: 1345/2000][D loss: 1.003363][G loss: 1.390887]\n",
            "[Epoch: 1346/2000][D loss: 1.004485][G loss: 1.438109]\n",
            "[Epoch: 1347/2000][D loss: 0.995869][G loss: 1.272025]\n",
            "[Epoch: 1348/2000][D loss: 1.013517][G loss: 1.375450]\n",
            "[Epoch: 1349/2000][D loss: 0.977297][G loss: 1.337148]\n",
            "[Epoch: 1350/2000][D loss: 1.088618][G loss: 1.443515]\n",
            "[Epoch: 1351/2000][D loss: 1.002895][G loss: 1.285273]\n",
            "[Epoch: 1352/2000][D loss: 1.040008][G loss: 1.376348]\n",
            "[Epoch: 1353/2000][D loss: 1.009862][G loss: 1.247260]\n",
            "[Epoch: 1354/2000][D loss: 1.078770][G loss: 1.396471]\n",
            "[Epoch: 1355/2000][D loss: 1.179942][G loss: 1.358442]\n",
            "[Epoch: 1356/2000][D loss: 1.072770][G loss: 1.215540]\n",
            "[Epoch: 1357/2000][D loss: 1.184872][G loss: 1.410488]\n",
            "[Epoch: 1358/2000][D loss: 1.073290][G loss: 1.298511]\n",
            "[Epoch: 1359/2000][D loss: 1.132832][G loss: 1.288089]\n",
            "[Epoch: 1360/2000][D loss: 1.029326][G loss: 1.336053]\n",
            "[Epoch: 1361/2000][D loss: 1.097056][G loss: 1.345476]\n",
            "[Epoch: 1362/2000][D loss: 0.984298][G loss: 1.364224]\n",
            "[Epoch: 1363/2000][D loss: 1.045630][G loss: 1.330153]\n",
            "[Epoch: 1364/2000][D loss: 0.984522][G loss: 1.353893]\n",
            "[Epoch: 1365/2000][D loss: 1.072082][G loss: 1.583309]\n",
            "[Epoch: 1366/2000][D loss: 1.056155][G loss: 1.488376]\n",
            "[Epoch: 1367/2000][D loss: 0.960087][G loss: 1.405532]\n",
            "[Epoch: 1368/2000][D loss: 0.957299][G loss: 1.339634]\n",
            "[Epoch: 1369/2000][D loss: 1.002621][G loss: 1.413838]\n",
            "[Epoch: 1370/2000][D loss: 1.136148][G loss: 1.471987]\n",
            "[Epoch: 1371/2000][D loss: 1.080346][G loss: 1.302018]\n",
            "[Epoch: 1372/2000][D loss: 1.136088][G loss: 1.387465]\n",
            "[Epoch: 1373/2000][D loss: 1.019324][G loss: 1.398616]\n",
            "[Epoch: 1374/2000][D loss: 1.073443][G loss: 1.244300]\n",
            "[Epoch: 1375/2000][D loss: 1.115884][G loss: 1.372434]\n",
            "[Epoch: 1376/2000][D loss: 1.001320][G loss: 1.272420]\n",
            "[Epoch: 1377/2000][D loss: 1.109814][G loss: 1.381212]\n",
            "[Epoch: 1378/2000][D loss: 0.992890][G loss: 1.297640]\n",
            "[Epoch: 1379/2000][D loss: 1.042055][G loss: 1.340268]\n",
            "[Epoch: 1380/2000][D loss: 1.139518][G loss: 1.308655]\n",
            "[Epoch: 1381/2000][D loss: 1.017015][G loss: 1.184794]\n",
            "[Epoch: 1382/2000][D loss: 1.053832][G loss: 1.246404]\n",
            "[Epoch: 1383/2000][D loss: 1.053781][G loss: 1.399130]\n",
            "[Epoch: 1384/2000][D loss: 0.969780][G loss: 1.257425]\n",
            "[Epoch: 1385/2000][D loss: 0.973069][G loss: 1.463217]\n",
            "[Epoch: 1386/2000][D loss: 1.178630][G loss: 1.247463]\n",
            "[Epoch: 1387/2000][D loss: 1.007592][G loss: 1.281373]\n",
            "[Epoch: 1388/2000][D loss: 1.097583][G loss: 1.278927]\n",
            "[Epoch: 1389/2000][D loss: 0.981828][G loss: 1.487838]\n",
            "[Epoch: 1390/2000][D loss: 1.033742][G loss: 1.532764]\n",
            "[Epoch: 1391/2000][D loss: 1.016468][G loss: 1.316749]\n",
            "[Epoch: 1392/2000][D loss: 1.141983][G loss: 1.335531]\n",
            "[Epoch: 1393/2000][D loss: 0.970021][G loss: 1.357821]\n",
            "[Epoch: 1394/2000][D loss: 1.001298][G loss: 1.326671]\n",
            "[Epoch: 1395/2000][D loss: 1.161923][G loss: 1.240255]\n",
            "[Epoch: 1396/2000][D loss: 0.992564][G loss: 1.402469]\n",
            "[Epoch: 1397/2000][D loss: 0.977177][G loss: 1.343952]\n",
            "[Epoch: 1398/2000][D loss: 1.064217][G loss: 1.316483]\n",
            "[Epoch: 1399/2000][D loss: 1.100810][G loss: 1.385389]\n",
            "[Epoch: 1400/2000][D loss: 1.026198][G loss: 1.562973]\n",
            "[Epoch: 1401/2000][D loss: 1.100026][G loss: 1.338753]\n",
            "[Epoch: 1402/2000][D loss: 1.020506][G loss: 1.398717]\n",
            "[Epoch: 1403/2000][D loss: 1.068003][G loss: 1.322976]\n",
            "[Epoch: 1404/2000][D loss: 1.032289][G loss: 1.331884]\n",
            "[Epoch: 1405/2000][D loss: 0.992865][G loss: 1.322078]\n",
            "[Epoch: 1406/2000][D loss: 1.039266][G loss: 1.494733]\n",
            "[Epoch: 1407/2000][D loss: 1.097895][G loss: 1.350876]\n",
            "[Epoch: 1408/2000][D loss: 1.007349][G loss: 1.240985]\n",
            "[Epoch: 1409/2000][D loss: 1.177599][G loss: 1.324100]\n",
            "[Epoch: 1410/2000][D loss: 1.095636][G loss: 1.234300]\n",
            "[Epoch: 1411/2000][D loss: 1.065639][G loss: 1.423664]\n",
            "[Epoch: 1412/2000][D loss: 0.954561][G loss: 1.425236]\n",
            "[Epoch: 1413/2000][D loss: 1.060806][G loss: 1.341807]\n",
            "[Epoch: 1414/2000][D loss: 1.058516][G loss: 1.355626]\n",
            "[Epoch: 1415/2000][D loss: 1.080912][G loss: 1.268254]\n",
            "[Epoch: 1416/2000][D loss: 1.040320][G loss: 1.328900]\n",
            "[Epoch: 1417/2000][D loss: 1.180456][G loss: 1.328275]\n",
            "[Epoch: 1418/2000][D loss: 0.985509][G loss: 1.165900]\n",
            "[Epoch: 1419/2000][D loss: 1.081231][G loss: 1.420654]\n",
            "[Epoch: 1420/2000][D loss: 0.973841][G loss: 1.212644]\n",
            "[Epoch: 1421/2000][D loss: 0.971757][G loss: 1.354178]\n",
            "[Epoch: 1422/2000][D loss: 0.996990][G loss: 1.287116]\n",
            "[Epoch: 1423/2000][D loss: 1.097268][G loss: 1.327615]\n",
            "[Epoch: 1424/2000][D loss: 1.016809][G loss: 1.444492]\n",
            "[Epoch: 1425/2000][D loss: 1.098991][G loss: 1.367340]\n",
            "[Epoch: 1426/2000][D loss: 0.941482][G loss: 1.448653]\n",
            "[Epoch: 1427/2000][D loss: 0.890409][G loss: 1.612387]\n",
            "[Epoch: 1428/2000][D loss: 1.062766][G loss: 1.349164]\n",
            "[Epoch: 1429/2000][D loss: 1.161254][G loss: 1.291936]\n",
            "[Epoch: 1430/2000][D loss: 1.103361][G loss: 1.206329]\n",
            "[Epoch: 1431/2000][D loss: 1.016926][G loss: 1.383819]\n",
            "[Epoch: 1432/2000][D loss: 0.946100][G loss: 1.409824]\n",
            "[Epoch: 1433/2000][D loss: 1.085896][G loss: 1.481322]\n",
            "[Epoch: 1434/2000][D loss: 0.964769][G loss: 1.399480]\n",
            "[Epoch: 1435/2000][D loss: 0.980536][G loss: 1.310958]\n",
            "[Epoch: 1436/2000][D loss: 1.078450][G loss: 1.316788]\n",
            "[Epoch: 1437/2000][D loss: 0.984767][G loss: 1.431144]\n",
            "[Epoch: 1438/2000][D loss: 1.080258][G loss: 1.291923]\n",
            "[Epoch: 1439/2000][D loss: 0.991578][G loss: 1.440625]\n",
            "[Epoch: 1440/2000][D loss: 0.978198][G loss: 1.422429]\n",
            "[Epoch: 1441/2000][D loss: 1.022806][G loss: 1.256716]\n",
            "[Epoch: 1442/2000][D loss: 1.070628][G loss: 1.346027]\n",
            "[Epoch: 1443/2000][D loss: 1.039964][G loss: 1.359122]\n",
            "[Epoch: 1444/2000][D loss: 1.074861][G loss: 1.424652]\n",
            "[Epoch: 1445/2000][D loss: 1.141303][G loss: 1.343261]\n",
            "[Epoch: 1446/2000][D loss: 0.941887][G loss: 1.342917]\n",
            "[Epoch: 1447/2000][D loss: 1.017801][G loss: 1.283153]\n",
            "[Epoch: 1448/2000][D loss: 1.026116][G loss: 1.293741]\n",
            "[Epoch: 1449/2000][D loss: 1.073219][G loss: 1.347741]\n",
            "[Epoch: 1450/2000][D loss: 1.017084][G loss: 1.401763]\n",
            "[Epoch: 1451/2000][D loss: 1.003595][G loss: 1.453173]\n",
            "[Epoch: 1452/2000][D loss: 1.063716][G loss: 1.277680]\n",
            "[Epoch: 1453/2000][D loss: 1.025039][G loss: 1.365297]\n",
            "[Epoch: 1454/2000][D loss: 1.077576][G loss: 1.414901]\n",
            "[Epoch: 1455/2000][D loss: 1.073739][G loss: 1.387312]\n",
            "[Epoch: 1456/2000][D loss: 1.004150][G loss: 1.483151]\n",
            "[Epoch: 1457/2000][D loss: 1.106606][G loss: 1.393887]\n",
            "[Epoch: 1458/2000][D loss: 1.017109][G loss: 1.286047]\n",
            "[Epoch: 1459/2000][D loss: 1.160027][G loss: 1.413773]\n",
            "[Epoch: 1460/2000][D loss: 1.025965][G loss: 1.312285]\n",
            "[Epoch: 1461/2000][D loss: 1.110155][G loss: 1.287356]\n",
            "[Epoch: 1462/2000][D loss: 1.038730][G loss: 1.377523]\n",
            "[Epoch: 1463/2000][D loss: 0.950703][G loss: 1.583769]\n",
            "[Epoch: 1464/2000][D loss: 1.024055][G loss: 1.321919]\n",
            "[Epoch: 1465/2000][D loss: 1.043162][G loss: 1.194329]\n",
            "[Epoch: 1466/2000][D loss: 1.011868][G loss: 1.348144]\n",
            "[Epoch: 1467/2000][D loss: 0.914165][G loss: 1.486384]\n",
            "[Epoch: 1468/2000][D loss: 1.024558][G loss: 1.406691]\n",
            "[Epoch: 1469/2000][D loss: 1.018519][G loss: 1.480187]\n",
            "[Epoch: 1470/2000][D loss: 0.955069][G loss: 1.354170]\n",
            "[Epoch: 1471/2000][D loss: 1.027064][G loss: 1.365738]\n",
            "[Epoch: 1472/2000][D loss: 1.063146][G loss: 1.193148]\n",
            "[Epoch: 1473/2000][D loss: 1.056345][G loss: 1.459329]\n",
            "[Epoch: 1474/2000][D loss: 1.003334][G loss: 1.381321]\n",
            "[Epoch: 1475/2000][D loss: 1.004622][G loss: 1.362732]\n",
            "[Epoch: 1476/2000][D loss: 1.005683][G loss: 1.400809]\n",
            "[Epoch: 1477/2000][D loss: 1.055778][G loss: 1.319524]\n",
            "[Epoch: 1478/2000][D loss: 0.963826][G loss: 1.355466]\n",
            "[Epoch: 1479/2000][D loss: 0.930627][G loss: 1.406245]\n",
            "[Epoch: 1480/2000][D loss: 0.998880][G loss: 1.306402]\n",
            "[Epoch: 1481/2000][D loss: 1.086099][G loss: 1.437779]\n",
            "[Epoch: 1482/2000][D loss: 1.053206][G loss: 1.409647]\n",
            "[Epoch: 1483/2000][D loss: 1.044735][G loss: 1.382613]\n",
            "[Epoch: 1484/2000][D loss: 1.032605][G loss: 1.282571]\n",
            "[Epoch: 1485/2000][D loss: 0.978711][G loss: 1.419438]\n",
            "[Epoch: 1486/2000][D loss: 0.941272][G loss: 1.385617]\n",
            "[Epoch: 1487/2000][D loss: 0.989207][G loss: 1.492545]\n",
            "[Epoch: 1488/2000][D loss: 1.151462][G loss: 1.232966]\n",
            "[Epoch: 1489/2000][D loss: 1.109315][G loss: 1.412577]\n",
            "[Epoch: 1490/2000][D loss: 0.977145][G loss: 1.396652]\n",
            "[Epoch: 1491/2000][D loss: 1.174860][G loss: 1.232294]\n",
            "[Epoch: 1492/2000][D loss: 1.155038][G loss: 1.277854]\n",
            "[Epoch: 1493/2000][D loss: 0.997242][G loss: 1.339047]\n",
            "[Epoch: 1494/2000][D loss: 1.000533][G loss: 1.308428]\n",
            "[Epoch: 1495/2000][D loss: 1.067507][G loss: 1.317196]\n",
            "[Epoch: 1496/2000][D loss: 0.956720][G loss: 1.352854]\n",
            "[Epoch: 1497/2000][D loss: 0.989089][G loss: 1.325904]\n",
            "[Epoch: 1498/2000][D loss: 1.104543][G loss: 1.338935]\n",
            "[Epoch: 1499/2000][D loss: 1.048273][G loss: 1.421999]\n",
            "[Epoch: 1500/2000][D loss: 0.972862][G loss: 1.472865]\n",
            "[Epoch: 1501/2000][D loss: 1.100913][G loss: 1.476697]\n",
            "[Epoch: 1502/2000][D loss: 1.051812][G loss: 1.538502]\n",
            "[Epoch: 1503/2000][D loss: 1.050255][G loss: 1.518656]\n",
            "[Epoch: 1504/2000][D loss: 1.095845][G loss: 1.432038]\n",
            "[Epoch: 1505/2000][D loss: 1.033061][G loss: 1.364538]\n",
            "[Epoch: 1506/2000][D loss: 1.102847][G loss: 1.429777]\n",
            "[Epoch: 1507/2000][D loss: 1.029611][G loss: 1.404564]\n",
            "[Epoch: 1508/2000][D loss: 0.963060][G loss: 1.256992]\n",
            "[Epoch: 1509/2000][D loss: 1.121818][G loss: 1.419713]\n",
            "[Epoch: 1510/2000][D loss: 1.026736][G loss: 1.246805]\n",
            "[Epoch: 1511/2000][D loss: 1.035001][G loss: 1.384108]\n",
            "[Epoch: 1512/2000][D loss: 0.956253][G loss: 1.371922]\n",
            "[Epoch: 1513/2000][D loss: 1.086998][G loss: 1.481238]\n",
            "[Epoch: 1514/2000][D loss: 0.961504][G loss: 1.412831]\n",
            "[Epoch: 1515/2000][D loss: 0.959151][G loss: 1.396182]\n",
            "[Epoch: 1516/2000][D loss: 1.040358][G loss: 1.292529]\n",
            "[Epoch: 1517/2000][D loss: 1.078344][G loss: 1.419135]\n",
            "[Epoch: 1518/2000][D loss: 1.059323][G loss: 1.223975]\n",
            "[Epoch: 1519/2000][D loss: 1.082480][G loss: 1.329368]\n",
            "[Epoch: 1520/2000][D loss: 1.040543][G loss: 1.367149]\n",
            "[Epoch: 1521/2000][D loss: 1.046536][G loss: 1.253649]\n",
            "[Epoch: 1522/2000][D loss: 1.066761][G loss: 1.277957]\n",
            "[Epoch: 1523/2000][D loss: 1.054590][G loss: 1.451068]\n",
            "[Epoch: 1524/2000][D loss: 0.934893][G loss: 1.490157]\n",
            "[Epoch: 1525/2000][D loss: 1.096085][G loss: 1.277949]\n",
            "[Epoch: 1526/2000][D loss: 1.006758][G loss: 1.458117]\n",
            "[Epoch: 1527/2000][D loss: 1.116228][G loss: 1.401149]\n",
            "[Epoch: 1528/2000][D loss: 1.140591][G loss: 1.288995]\n",
            "[Epoch: 1529/2000][D loss: 1.038662][G loss: 1.397985]\n",
            "[Epoch: 1530/2000][D loss: 1.082853][G loss: 1.256197]\n",
            "[Epoch: 1531/2000][D loss: 1.056261][G loss: 1.490516]\n",
            "[Epoch: 1532/2000][D loss: 1.123874][G loss: 1.477210]\n",
            "[Epoch: 1533/2000][D loss: 0.979089][G loss: 1.394552]\n",
            "[Epoch: 1534/2000][D loss: 1.049096][G loss: 1.436572]\n",
            "[Epoch: 1535/2000][D loss: 1.149318][G loss: 1.478284]\n",
            "[Epoch: 1536/2000][D loss: 1.142783][G loss: 1.370568]\n",
            "[Epoch: 1537/2000][D loss: 1.147195][G loss: 1.363371]\n",
            "[Epoch: 1538/2000][D loss: 1.055601][G loss: 1.395340]\n",
            "[Epoch: 1539/2000][D loss: 0.970085][G loss: 1.448696]\n",
            "[Epoch: 1540/2000][D loss: 0.934976][G loss: 1.405904]\n",
            "[Epoch: 1541/2000][D loss: 0.996016][G loss: 1.414376]\n",
            "[Epoch: 1542/2000][D loss: 1.008276][G loss: 1.421115]\n",
            "[Epoch: 1543/2000][D loss: 0.981453][G loss: 1.398414]\n",
            "[Epoch: 1544/2000][D loss: 1.032837][G loss: 1.431937]\n",
            "[Epoch: 1545/2000][D loss: 0.881799][G loss: 1.447129]\n",
            "[Epoch: 1546/2000][D loss: 1.021129][G loss: 1.306174]\n",
            "[Epoch: 1547/2000][D loss: 0.954000][G loss: 1.446729]\n",
            "[Epoch: 1548/2000][D loss: 0.984604][G loss: 1.521464]\n",
            "[Epoch: 1549/2000][D loss: 0.989660][G loss: 1.652175]\n",
            "[Epoch: 1550/2000][D loss: 1.073689][G loss: 1.390783]\n",
            "[Epoch: 1551/2000][D loss: 0.966201][G loss: 1.347349]\n",
            "[Epoch: 1552/2000][D loss: 0.987573][G loss: 1.520917]\n",
            "[Epoch: 1553/2000][D loss: 1.079396][G loss: 1.353990]\n",
            "[Epoch: 1554/2000][D loss: 1.092652][G loss: 1.484026]\n",
            "[Epoch: 1555/2000][D loss: 0.939394][G loss: 1.547722]\n",
            "[Epoch: 1556/2000][D loss: 1.040316][G loss: 1.379130]\n",
            "[Epoch: 1557/2000][D loss: 1.095164][G loss: 1.214046]\n",
            "[Epoch: 1558/2000][D loss: 1.109208][G loss: 1.408907]\n",
            "[Epoch: 1559/2000][D loss: 0.986057][G loss: 1.136443]\n",
            "[Epoch: 1560/2000][D loss: 1.014780][G loss: 1.406405]\n",
            "[Epoch: 1561/2000][D loss: 1.032107][G loss: 1.448477]\n",
            "[Epoch: 1562/2000][D loss: 1.011323][G loss: 1.358870]\n",
            "[Epoch: 1563/2000][D loss: 1.069453][G loss: 1.355174]\n",
            "[Epoch: 1564/2000][D loss: 1.115295][G loss: 1.332844]\n",
            "[Epoch: 1565/2000][D loss: 1.007479][G loss: 1.445105]\n",
            "[Epoch: 1566/2000][D loss: 1.015891][G loss: 1.288674]\n",
            "[Epoch: 1567/2000][D loss: 1.129224][G loss: 1.442062]\n",
            "[Epoch: 1568/2000][D loss: 1.114634][G loss: 1.268251]\n",
            "[Epoch: 1569/2000][D loss: 0.965435][G loss: 1.230552]\n",
            "[Epoch: 1570/2000][D loss: 1.036220][G loss: 1.503980]\n",
            "[Epoch: 1571/2000][D loss: 0.999054][G loss: 1.211430]\n",
            "[Epoch: 1572/2000][D loss: 1.030156][G loss: 1.454711]\n",
            "[Epoch: 1573/2000][D loss: 0.954784][G loss: 1.380070]\n",
            "[Epoch: 1574/2000][D loss: 0.962709][G loss: 1.450073]\n",
            "[Epoch: 1575/2000][D loss: 1.104959][G loss: 1.310212]\n",
            "[Epoch: 1576/2000][D loss: 1.023218][G loss: 1.375091]\n",
            "[Epoch: 1577/2000][D loss: 1.103688][G loss: 1.402958]\n",
            "[Epoch: 1578/2000][D loss: 1.068923][G loss: 1.445981]\n",
            "[Epoch: 1579/2000][D loss: 1.053060][G loss: 1.513191]\n",
            "[Epoch: 1580/2000][D loss: 0.993421][G loss: 1.381594]\n",
            "[Epoch: 1581/2000][D loss: 1.062284][G loss: 1.354884]\n",
            "[Epoch: 1582/2000][D loss: 1.063182][G loss: 1.434744]\n",
            "[Epoch: 1583/2000][D loss: 1.059252][G loss: 1.420138]\n",
            "[Epoch: 1584/2000][D loss: 1.106629][G loss: 1.488017]\n",
            "[Epoch: 1585/2000][D loss: 1.132210][G loss: 1.396558]\n",
            "[Epoch: 1586/2000][D loss: 1.024613][G loss: 1.388369]\n",
            "[Epoch: 1587/2000][D loss: 1.044172][G loss: 1.437160]\n",
            "[Epoch: 1588/2000][D loss: 0.943752][G loss: 1.442338]\n",
            "[Epoch: 1589/2000][D loss: 1.122630][G loss: 1.188199]\n",
            "[Epoch: 1590/2000][D loss: 1.016753][G loss: 1.423128]\n",
            "[Epoch: 1591/2000][D loss: 1.108007][G loss: 1.236120]\n",
            "[Epoch: 1592/2000][D loss: 0.905981][G loss: 1.327257]\n",
            "[Epoch: 1593/2000][D loss: 1.050545][G loss: 1.270835]\n",
            "[Epoch: 1594/2000][D loss: 1.038656][G loss: 1.410750]\n",
            "[Epoch: 1595/2000][D loss: 1.076517][G loss: 1.425446]\n",
            "[Epoch: 1596/2000][D loss: 1.150264][G loss: 1.469851]\n",
            "[Epoch: 1597/2000][D loss: 0.983224][G loss: 1.488317]\n",
            "[Epoch: 1598/2000][D loss: 1.087764][G loss: 1.376797]\n",
            "[Epoch: 1599/2000][D loss: 1.129784][G loss: 1.414049]\n",
            "[Epoch: 1600/2000][D loss: 0.985523][G loss: 1.511149]\n",
            "[Epoch: 1601/2000][D loss: 0.940001][G loss: 1.381844]\n",
            "[Epoch: 1602/2000][D loss: 1.019728][G loss: 1.435031]\n",
            "[Epoch: 1603/2000][D loss: 0.975625][G loss: 1.453769]\n",
            "[Epoch: 1604/2000][D loss: 1.101513][G loss: 1.480672]\n",
            "[Epoch: 1605/2000][D loss: 1.062939][G loss: 1.425606]\n",
            "[Epoch: 1606/2000][D loss: 0.998779][G loss: 1.404354]\n",
            "[Epoch: 1607/2000][D loss: 1.000064][G loss: 1.424147]\n",
            "[Epoch: 1608/2000][D loss: 1.093629][G loss: 1.439512]\n",
            "[Epoch: 1609/2000][D loss: 1.045121][G loss: 1.481788]\n",
            "[Epoch: 1610/2000][D loss: 1.024160][G loss: 1.239329]\n",
            "[Epoch: 1611/2000][D loss: 1.086720][G loss: 1.395544]\n",
            "[Epoch: 1612/2000][D loss: 1.157514][G loss: 1.408433]\n",
            "[Epoch: 1613/2000][D loss: 1.063382][G loss: 1.274060]\n",
            "[Epoch: 1614/2000][D loss: 0.996317][G loss: 1.399911]\n",
            "[Epoch: 1615/2000][D loss: 1.034425][G loss: 1.535512]\n",
            "[Epoch: 1616/2000][D loss: 1.040696][G loss: 1.460311]\n",
            "[Epoch: 1617/2000][D loss: 0.994506][G loss: 1.341694]\n",
            "[Epoch: 1618/2000][D loss: 1.022925][G loss: 1.542695]\n",
            "[Epoch: 1619/2000][D loss: 1.105898][G loss: 1.381890]\n",
            "[Epoch: 1620/2000][D loss: 1.119066][G loss: 1.412606]\n",
            "[Epoch: 1621/2000][D loss: 1.063767][G loss: 1.476761]\n",
            "[Epoch: 1622/2000][D loss: 1.152352][G loss: 1.396376]\n",
            "[Epoch: 1623/2000][D loss: 1.029141][G loss: 1.349945]\n",
            "[Epoch: 1624/2000][D loss: 1.070738][G loss: 1.367922]\n",
            "[Epoch: 1625/2000][D loss: 0.967756][G loss: 1.264278]\n",
            "[Epoch: 1626/2000][D loss: 1.087749][G loss: 1.408529]\n",
            "[Epoch: 1627/2000][D loss: 0.981023][G loss: 1.244641]\n",
            "[Epoch: 1628/2000][D loss: 1.103438][G loss: 1.523595]\n",
            "[Epoch: 1629/2000][D loss: 1.007036][G loss: 1.336358]\n",
            "[Epoch: 1630/2000][D loss: 0.989641][G loss: 1.265009]\n",
            "[Epoch: 1631/2000][D loss: 1.163487][G loss: 1.381680]\n",
            "[Epoch: 1632/2000][D loss: 1.154239][G loss: 1.228008]\n",
            "[Epoch: 1633/2000][D loss: 1.082690][G loss: 1.343007]\n",
            "[Epoch: 1634/2000][D loss: 1.073170][G loss: 1.443933]\n",
            "[Epoch: 1635/2000][D loss: 1.090290][G loss: 1.292304]\n",
            "[Epoch: 1636/2000][D loss: 1.060454][G loss: 1.286444]\n",
            "[Epoch: 1637/2000][D loss: 1.028962][G loss: 1.430211]\n",
            "[Epoch: 1638/2000][D loss: 1.074803][G loss: 1.314628]\n",
            "[Epoch: 1639/2000][D loss: 0.963466][G loss: 1.616104]\n",
            "[Epoch: 1640/2000][D loss: 1.072519][G loss: 1.472127]\n",
            "[Epoch: 1641/2000][D loss: 1.040003][G loss: 1.555820]\n",
            "[Epoch: 1642/2000][D loss: 0.971989][G loss: 1.474326]\n",
            "[Epoch: 1643/2000][D loss: 1.081416][G loss: 1.488603]\n",
            "[Epoch: 1644/2000][D loss: 1.130861][G loss: 1.506575]\n",
            "[Epoch: 1645/2000][D loss: 1.084007][G loss: 1.417924]\n",
            "[Epoch: 1646/2000][D loss: 1.048011][G loss: 1.417731]\n",
            "[Epoch: 1647/2000][D loss: 0.947487][G loss: 1.307752]\n",
            "[Epoch: 1648/2000][D loss: 0.984723][G loss: 1.437514]\n",
            "[Epoch: 1649/2000][D loss: 1.060776][G loss: 1.380030]\n",
            "[Epoch: 1650/2000][D loss: 1.069827][G loss: 1.204761]\n",
            "[Epoch: 1651/2000][D loss: 1.159635][G loss: 1.293767]\n",
            "[Epoch: 1652/2000][D loss: 1.048203][G loss: 1.376923]\n",
            "[Epoch: 1653/2000][D loss: 1.025876][G loss: 1.454249]\n",
            "[Epoch: 1654/2000][D loss: 1.204673][G loss: 1.335836]\n",
            "[Epoch: 1655/2000][D loss: 1.130169][G loss: 1.250087]\n",
            "[Epoch: 1656/2000][D loss: 1.044418][G loss: 1.344603]\n",
            "[Epoch: 1657/2000][D loss: 1.242171][G loss: 1.297219]\n",
            "[Epoch: 1658/2000][D loss: 0.945017][G loss: 1.422346]\n",
            "[Epoch: 1659/2000][D loss: 1.148015][G loss: 1.503448]\n",
            "[Epoch: 1660/2000][D loss: 1.107121][G loss: 1.293986]\n",
            "[Epoch: 1661/2000][D loss: 1.061359][G loss: 1.303192]\n",
            "[Epoch: 1662/2000][D loss: 1.148903][G loss: 1.442382]\n",
            "[Epoch: 1663/2000][D loss: 0.972700][G loss: 1.369786]\n",
            "[Epoch: 1664/2000][D loss: 1.030868][G loss: 1.349927]\n",
            "[Epoch: 1665/2000][D loss: 1.175825][G loss: 1.267377]\n",
            "[Epoch: 1666/2000][D loss: 1.048144][G loss: 1.322668]\n",
            "[Epoch: 1667/2000][D loss: 0.941132][G loss: 1.384025]\n",
            "[Epoch: 1668/2000][D loss: 1.107777][G loss: 1.356195]\n",
            "[Epoch: 1669/2000][D loss: 1.038771][G loss: 1.281737]\n",
            "[Epoch: 1670/2000][D loss: 0.990143][G loss: 1.259573]\n",
            "[Epoch: 1671/2000][D loss: 1.039903][G loss: 1.441798]\n",
            "[Epoch: 1672/2000][D loss: 1.032203][G loss: 1.279321]\n",
            "[Epoch: 1673/2000][D loss: 0.966813][G loss: 1.515330]\n",
            "[Epoch: 1674/2000][D loss: 1.092360][G loss: 1.215293]\n",
            "[Epoch: 1675/2000][D loss: 0.900025][G loss: 1.399200]\n",
            "[Epoch: 1676/2000][D loss: 1.064824][G loss: 1.390604]\n",
            "[Epoch: 1677/2000][D loss: 1.033552][G loss: 1.347556]\n",
            "[Epoch: 1678/2000][D loss: 1.081251][G loss: 1.241524]\n",
            "[Epoch: 1679/2000][D loss: 1.188810][G loss: 1.285544]\n",
            "[Epoch: 1680/2000][D loss: 1.108934][G loss: 1.183151]\n",
            "[Epoch: 1681/2000][D loss: 1.208702][G loss: 1.298329]\n",
            "[Epoch: 1682/2000][D loss: 1.050514][G loss: 1.478026]\n",
            "[Epoch: 1683/2000][D loss: 0.986843][G loss: 1.388666]\n",
            "[Epoch: 1684/2000][D loss: 0.899470][G loss: 1.516082]\n",
            "[Epoch: 1685/2000][D loss: 1.064181][G loss: 1.147944]\n",
            "[Epoch: 1686/2000][D loss: 1.054972][G loss: 1.275427]\n",
            "[Epoch: 1687/2000][D loss: 1.054791][G loss: 1.354009]\n",
            "[Epoch: 1688/2000][D loss: 1.099372][G loss: 1.361119]\n",
            "[Epoch: 1689/2000][D loss: 1.080892][G loss: 1.463904]\n",
            "[Epoch: 1690/2000][D loss: 1.015591][G loss: 1.426477]\n",
            "[Epoch: 1691/2000][D loss: 1.059800][G loss: 1.345766]\n",
            "[Epoch: 1692/2000][D loss: 0.979996][G loss: 1.440261]\n",
            "[Epoch: 1693/2000][D loss: 1.043747][G loss: 1.315834]\n",
            "[Epoch: 1694/2000][D loss: 1.030903][G loss: 1.338017]\n",
            "[Epoch: 1695/2000][D loss: 1.045498][G loss: 1.349345]\n",
            "[Epoch: 1696/2000][D loss: 1.116447][G loss: 1.345416]\n",
            "[Epoch: 1697/2000][D loss: 1.169461][G loss: 1.375885]\n",
            "[Epoch: 1698/2000][D loss: 1.112837][G loss: 1.406028]\n",
            "[Epoch: 1699/2000][D loss: 1.017790][G loss: 1.366769]\n",
            "[Epoch: 1700/2000][D loss: 1.169389][G loss: 1.240418]\n",
            "[Epoch: 1701/2000][D loss: 1.210268][G loss: 1.438909]\n",
            "[Epoch: 1702/2000][D loss: 1.109236][G loss: 1.377148]\n",
            "[Epoch: 1703/2000][D loss: 1.147217][G loss: 1.400679]\n",
            "[Epoch: 1704/2000][D loss: 1.003674][G loss: 1.420441]\n",
            "[Epoch: 1705/2000][D loss: 1.089502][G loss: 1.333706]\n",
            "[Epoch: 1706/2000][D loss: 1.139979][G loss: 1.240013]\n",
            "[Epoch: 1707/2000][D loss: 1.078269][G loss: 1.482917]\n",
            "[Epoch: 1708/2000][D loss: 1.066237][G loss: 1.262884]\n",
            "[Epoch: 1709/2000][D loss: 1.034028][G loss: 1.515623]\n",
            "[Epoch: 1710/2000][D loss: 0.995161][G loss: 1.439616]\n",
            "[Epoch: 1711/2000][D loss: 1.053340][G loss: 1.165991]\n",
            "[Epoch: 1712/2000][D loss: 0.979348][G loss: 1.292597]\n",
            "[Epoch: 1713/2000][D loss: 1.039227][G loss: 1.396103]\n",
            "[Epoch: 1714/2000][D loss: 1.205805][G loss: 1.328155]\n",
            "[Epoch: 1715/2000][D loss: 1.067636][G loss: 1.349224]\n",
            "[Epoch: 1716/2000][D loss: 0.919179][G loss: 1.338603]\n",
            "[Epoch: 1717/2000][D loss: 1.144741][G loss: 1.191944]\n",
            "[Epoch: 1718/2000][D loss: 1.160775][G loss: 1.312478]\n",
            "[Epoch: 1719/2000][D loss: 1.164684][G loss: 1.465802]\n",
            "[Epoch: 1720/2000][D loss: 1.076419][G loss: 1.307315]\n",
            "[Epoch: 1721/2000][D loss: 1.043495][G loss: 1.358877]\n",
            "[Epoch: 1722/2000][D loss: 1.112276][G loss: 1.236087]\n",
            "[Epoch: 1723/2000][D loss: 1.013500][G loss: 1.427903]\n",
            "[Epoch: 1724/2000][D loss: 1.031937][G loss: 1.374594]\n",
            "[Epoch: 1725/2000][D loss: 0.985977][G loss: 1.325517]\n",
            "[Epoch: 1726/2000][D loss: 1.166702][G loss: 1.246830]\n",
            "[Epoch: 1727/2000][D loss: 1.116561][G loss: 1.225174]\n",
            "[Epoch: 1728/2000][D loss: 1.104050][G loss: 1.277173]\n",
            "[Epoch: 1729/2000][D loss: 1.048662][G loss: 1.427060]\n",
            "[Epoch: 1730/2000][D loss: 0.928335][G loss: 1.281334]\n",
            "[Epoch: 1731/2000][D loss: 1.094316][G loss: 1.462485]\n",
            "[Epoch: 1732/2000][D loss: 1.024138][G loss: 1.403373]\n",
            "[Epoch: 1733/2000][D loss: 0.950689][G loss: 1.311005]\n",
            "[Epoch: 1734/2000][D loss: 1.056170][G loss: 1.389583]\n",
            "[Epoch: 1735/2000][D loss: 1.063169][G loss: 1.409985]\n",
            "[Epoch: 1736/2000][D loss: 1.094055][G loss: 1.477299]\n",
            "[Epoch: 1737/2000][D loss: 1.084338][G loss: 1.418011]\n",
            "[Epoch: 1738/2000][D loss: 1.091415][G loss: 1.412568]\n",
            "[Epoch: 1739/2000][D loss: 1.102795][G loss: 1.238193]\n",
            "[Epoch: 1740/2000][D loss: 1.084790][G loss: 1.347766]\n",
            "[Epoch: 1741/2000][D loss: 1.057837][G loss: 1.290658]\n",
            "[Epoch: 1742/2000][D loss: 1.081028][G loss: 1.312295]\n",
            "[Epoch: 1743/2000][D loss: 1.118313][G loss: 1.343478]\n",
            "[Epoch: 1744/2000][D loss: 1.186942][G loss: 1.368867]\n",
            "[Epoch: 1745/2000][D loss: 1.136573][G loss: 1.247401]\n",
            "[Epoch: 1746/2000][D loss: 0.992188][G loss: 1.309971]\n",
            "[Epoch: 1747/2000][D loss: 1.203664][G loss: 1.367058]\n",
            "[Epoch: 1748/2000][D loss: 1.269137][G loss: 1.201940]\n",
            "[Epoch: 1749/2000][D loss: 1.014420][G loss: 1.378844]\n",
            "[Epoch: 1750/2000][D loss: 1.147318][G loss: 1.303132]\n",
            "[Epoch: 1751/2000][D loss: 1.123457][G loss: 1.349071]\n",
            "[Epoch: 1752/2000][D loss: 1.051764][G loss: 1.328533]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLMl4RiIaPRA",
        "colab_type": "code",
        "outputId": "757bfde5-e284-4a9f-cca1-a3e47043ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generator.load_state_dict(torch.load(\"%sgenerator.pth\" % (outputdir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRm7RFtZCXq9",
        "outputId": "76eab536-a6a2-4efe-c8f2-27ced153babb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "X, gen_labels = dataset.__getitem__(3)\n",
        "gen_labels = Variable(gen_labels.type(FT_a))\n",
        "noise = Variable(FT_a(np.random.normal(0, 1, (LAT_DIM, ))))\n",
        "gen_imgs = X\n",
        "gen_imgs = Variable(gen_imgs.type(FT_a))\n",
        "gen_imgs = gen_imgs.view(1, 3, 32, 32)\n",
        "print(gen_imgs.shape)\n",
        "print(gen_labels)\n",
        "\n",
        "\n",
        "for i in range(1,10, 1):\n",
        "    \n",
        "    gen_labels[6]=7-1.5*i # from female to male (male is negative)\n",
        "    #gen_labels[3]=4-0.8*i # from not smile to smile (smile is negative)\n",
        "    #gen_labels[10]=4-0.8*i    #from mouth closed to non mouth closed(mouth closed is negative)\n",
        "    \n",
        "    #gen_labels[1]=-7+1.5*i\n",
        "    #gen_labels[2]=-7+i\n",
        "    #gen_labels[3] = -7 + i*1.4\n",
        "    #gen_labels[10] = -5 + i*1\n",
        "    #gen_labels[6] = -5 + i\n",
        "   # gen_labels[7] = 2 - i*0.4\n",
        "    #gen_labels[8] = 3 - i*0.6\n",
        "    #gen_labels[33]=7-i*1.4\n",
        "\n",
        "    print(gen_labels)\n",
        "    img = generator(noise, gen_labels, 1)\n",
        "    print(img.shape)\n",
        "    gen_imgs = torch.cat((gen_imgs, img), -1)\n",
        "    vutils.save_image(gen_imgs, 'final_gen_sample_%d.png' % i, normalize=True)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "plt.imshow(X.permute(1, 2, 0))\n",
        "plt.imshow(gen_imgs.cpu().detach()[0].permute(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -1.8032, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700,  5.5000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700,  4.0000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700,  2.5000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700,  1.0000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -0.5000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -2.0000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -3.5000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -5.0000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n",
            "tensor([ 1.6870,  2.3160, -0.8239, -1.4251, -1.1007, -0.9700, -6.5000, -0.9561,\n",
            "         0.5468, -1.8568, -0.7860], device='cuda:0')\n",
            "torch.Size([1, 3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff119769e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABECAYAAACYhW4wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPviWCRNdtyY",
        "colab_type": "text"
      },
      "source": [
        "**Doing KNN test for overfitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD8T6BAwdc-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 16\n",
        "k=1 \n",
        "\n",
        "# Using Knn to find nearest Neighbor\n",
        "dataset2 = LFWCrop(lfwroot, transform=transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "\n",
        "for i in range(0, dataset2.__len__()):\n",
        "    image, _= dataset2.__getitem__(i)\n",
        "    image = image.flatten().numpy()\n",
        "\n",
        "    X_train.append(image)\n",
        "\n",
        "nnCls = NearestNeighbors(n_neighbors=k, p=1)\n",
        "nnCls.fit(X_train)\n",
        "\n",
        "def findKNNeighbors(myImage, nnCls=nnCls, img_size = 32):\n",
        "\n",
        "    distances, indeces = nnCls.kneighbors([myImage.flatten()])\n",
        "\n",
        "    for index in indeces[0]:\n",
        "        nearImage, _ = dataset.__getitem__(index)\n",
        "\n",
        "    return indeces[0]\n",
        "\n",
        "noise = Variable(FT_a(np.random.normal(0, 1, (LAT_DIM, ))))\n",
        "\n",
        "# we consider \"overfitted\" a generated image that has as nearest neighbor\n",
        "# the image from which the label that generated it was taken from\n",
        "\n",
        "overfitted = 0\n",
        "print(\"Calculating overfitting:\")\n",
        "  \n",
        "for i in range(dataset.__len__()):\n",
        "  origImg, label = dataset.__getitem__(i)\n",
        "  \n",
        "  label = Variable(label.type(FT_a))\n",
        "  # generate false image\n",
        "  gen_img = generator(noise, label, 1)\n",
        "  gen_img = gen_img.view(32, 32, 3).cpu()\n",
        "  gen_transform = transforms.Compose([\n",
        "          transforms.ToPILImage(),\n",
        "          transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
        "          transforms.Grayscale(),\n",
        "          transforms.ToTensor()\n",
        "      ])\n",
        "  \n",
        "  gen_img = gen_transform(gen_img)\n",
        "  nearest = findKNNeighbors(gen_img.numpy(), img_size=IMG_SIZE)\n",
        "  \n",
        "  \n",
        "  if nearest[0] == i: \n",
        "    overfitted += 1\n",
        "  else:\n",
        "    print(\"Percentage: %f\" % (i/dataset.__len__()), end = '\\r')\n",
        "\n",
        "# how many images overfitted in the whole dataset?\n",
        "overfit_ratio = overfitted/dataset.__len__()\n",
        "print(\"Overfit Ratio = %f\" % (overfit_ratio))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXBO0USy3nF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(overfitted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-peMKhoyQlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.load_state_dict(torch.load(PATH_g))\n",
        "generator.eval()\n",
        "\n",
        "discriminator.load_state_dict(torch.load(PATH_d))\n",
        "discriminator.eval()\n",
        "\n",
        "gen_labels = Variable(FT_a(2*np.random.standard_normal(size=(BATCH_SIZE, N_CLASSES))))\n",
        "noise = Variable(FT_a(np.random.normal(0, 1, (BATCH_SIZE, LAT_DIM))))\n",
        "\n",
        "gen_imgs = generator(noise, gen_labels, BATCH_SIZE)\n",
        "vutils.save_image(gen_imgs, '%s/final_gen_sample.png' % outputdir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}